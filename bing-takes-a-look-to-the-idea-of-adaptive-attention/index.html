<!DOCTYPE html>
<html lang="es">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="robots" content="max-image-preview:large">
<script id="cookieyes" type="text/javascript" src="https://cdn-cookieyes.com/client_data/d6d0c2bcb5ebe975905eb0ec/script.js"></script><title>Bing takes a look to the idea of adaptive attention. &#8211; Incidencias</title>
<link rel="dns-prefetch" href="/blog">
<link rel="alternate" type="application/rss+xml" title="Incidencias &raquo; Feed" href="/blog/feed/">
<link rel="alternate" type="application/rss+xml" title="Incidencias &raquo; Feed de los comentarios" href="/blog/comments/feed/">
<link rel="alternate" type="application/rss+xml" title="Incidencias &raquo; Comentario Bing takes a look to the idea of adaptive attention. del feed" href="/blog/bing-takes-a-look-to-the-idea-of-adaptive-attention/feed/">
<script>
window._wpemojiSettings = {"baseUrl":"https:\/\/s.w.org\/images\/core\/emoji\/15.0.3\/72x72\/","ext":".png","svgUrl":"https:\/\/s.w.org\/images\/core\/emoji\/15.0.3\/svg\/","svgExt":".svg","source":{"concatemoji":"\/blog\/wp-includes\/js\/wp-emoji-release.min.js?ver=6.5.3"}};
/*! This file is auto-generated */
!function(i,n){var o,s,e;function c(e){try{var t={supportTests:e,timestamp:(new Date).valueOf()};sessionStorage.setItem(o,JSON.stringify(t))}catch(e){}}function p(e,t,n){e.clearRect(0,0,e.canvas.width,e.canvas.height),e.fillText(t,0,0);var t=new Uint32Array(e.getImageData(0,0,e.canvas.width,e.canvas.height).data),r=(e.clearRect(0,0,e.canvas.width,e.canvas.height),e.fillText(n,0,0),new Uint32Array(e.getImageData(0,0,e.canvas.width,e.canvas.height).data));return t.every(function(e,t){return e===r[t]})}function u(e,t,n){switch(t){case"flag":return n(e,"üè≥Ô∏è‚Äç‚ößÔ∏è","üè≥Ô∏è‚Äã‚ößÔ∏è")?!1:!n(e,"üá∫üá≥","üá∫‚Äãüá≥")&&!n(e,"üè¥Û†ÅßÛ†Å¢Û†Å•Û†ÅÆÛ†ÅßÛ†Åø","üè¥‚ÄãÛ†Åß‚ÄãÛ†Å¢‚ÄãÛ†Å•‚ÄãÛ†ÅÆ‚ÄãÛ†Åß‚ÄãÛ†Åø");case"emoji":return!n(e,"üê¶‚Äç‚¨õ","üê¶‚Äã‚¨õ")}return!1}function f(e,t,n){var r="undefined"!=typeof WorkerGlobalScope&&self instanceof WorkerGlobalScope?new OffscreenCanvas(300,150):i.createElement("canvas"),a=r.getContext("2d",{willReadFrequently:!0}),o=(a.textBaseline="top",a.font="600 32px Arial",{});return e.forEach(function(e){o[e]=t(a,e,n)}),o}function t(e){var t=i.createElement("script");t.src=e,t.defer=!0,i.head.appendChild(t)}"undefined"!=typeof Promise&&(o="wpEmojiSettingsSupports",s=["flag","emoji"],n.supports={everything:!0,everythingExceptFlag:!0},e=new Promise(function(e){i.addEventListener("DOMContentLoaded",e,{once:!0})}),new Promise(function(t){var n=function(){try{var e=JSON.parse(sessionStorage.getItem(o));if("object"==typeof e&&"number"==typeof e.timestamp&&(new Date).valueOf()<e.timestamp+604800&&"object"==typeof e.supportTests)return e.supportTests}catch(e){}return null}();if(!n){if("undefined"!=typeof Worker&&"undefined"!=typeof OffscreenCanvas&&"undefined"!=typeof URL&&URL.createObjectURL&&"undefined"!=typeof Blob)try{var e="postMessage("+f.toString()+"("+[JSON.stringify(s),u.toString(),p.toString()].join(",")+"));",r=new Blob([e],{type:"text/javascript"}),a=new Worker(URL.createObjectURL(r),{name:"wpTestEmojiSupports"});return void(a.onmessage=function(e){c(n=e.data),a.terminate(),t(n)})}catch(e){}c(n=f(s,u,p))}t(n)}).then(function(e){for(var t in e)n.supports[t]=e[t],n.supports.everything=n.supports.everything&&n.supports[t],"flag"!==t&&(n.supports.everythingExceptFlag=n.supports.everythingExceptFlag&&n.supports[t]);n.supports.everythingExceptFlag=n.supports.everythingExceptFlag&&!n.supports.flag,n.DOMReady=!1,n.readyCallback=function(){n.DOMReady=!0}}).then(function(){return e}).then(function(){var e;n.supports.everything||(n.readyCallback(),(e=n.source||{}).concatemoji?t(e.concatemoji):e.wpemoji&&e.twemoji&&(t(e.twemoji),t(e.wpemoji)))}))}((window,document),window._wpemojiSettings);
</script>
<style id="wp-block-site-logo-inline-css">.wp-block-site-logo{box-sizing:border-box;line-height:0}.wp-block-site-logo a{display:inline-block;line-height:0}.wp-block-site-logo.is-default-size img{height:auto;width:120px}.wp-block-site-logo img{height:auto;max-width:100%}.wp-block-site-logo a,.wp-block-site-logo img{border-radius:inherit}.wp-block-site-logo.aligncenter{margin-left:auto;margin-right:auto;text-align:center}.wp-block-site-logo.is-style-rounded{border-radius:9999px}</style>
<style id="wp-block-site-title-inline-css">.wp-block-site-title a{color:inherit}</style>
<style id="wp-block-group-inline-css">.wp-block-group{box-sizing:border-box}</style>
<style id="wp-block-page-list-inline-css">.wp-block-navigation .wp-block-page-list{align-items:var(--navigation-layout-align,initial);background-color:inherit;display:flex;flex-direction:var(--navigation-layout-direction,initial);flex-wrap:var(--navigation-layout-wrap,wrap);justify-content:var(--navigation-layout-justify,initial)}.wp-block-navigation .wp-block-navigation-item{background-color:inherit}</style>
<link rel="stylesheet" id="wp-block-navigation-css" href="/blog/wp-includes/blocks/navigation/style.min.css?ver=6.5.3" media="all">
<style id="wp-block-post-featured-image-inline-css">.wp-block-post-featured-image{margin-left:0;margin-right:0}.wp-block-post-featured-image a{display:block;height:100%}.wp-block-post-featured-image img{box-sizing:border-box;height:auto;max-width:100%;vertical-align:bottom;width:100%}.wp-block-post-featured-image.alignfull img,.wp-block-post-featured-image.alignwide img{width:100%}.wp-block-post-featured-image .wp-block-post-featured-image__overlay.has-background-dim{background-color:#000;inset:0;position:absolute}.wp-block-post-featured-image{position:relative}.wp-block-post-featured-image .wp-block-post-featured-image__overlay.has-background-gradient{background-color:initial}.wp-block-post-featured-image .wp-block-post-featured-image__overlay.has-background-dim-0{opacity:0}.wp-block-post-featured-image .wp-block-post-featured-image__overlay.has-background-dim-10{opacity:.1}.wp-block-post-featured-image .wp-block-post-featured-image__overlay.has-background-dim-20{opacity:.2}.wp-block-post-featured-image .wp-block-post-featured-image__overlay.has-background-dim-30{opacity:.3}.wp-block-post-featured-image .wp-block-post-featured-image__overlay.has-background-dim-40{opacity:.4}.wp-block-post-featured-image .wp-block-post-featured-image__overlay.has-background-dim-50{opacity:.5}.wp-block-post-featured-image .wp-block-post-featured-image__overlay.has-background-dim-60{opacity:.6}.wp-block-post-featured-image .wp-block-post-featured-image__overlay.has-background-dim-70{opacity:.7}.wp-block-post-featured-image .wp-block-post-featured-image__overlay.has-background-dim-80{opacity:.8}.wp-block-post-featured-image .wp-block-post-featured-image__overlay.has-background-dim-90{opacity:.9}.wp-block-post-featured-image .wp-block-post-featured-image__overlay.has-background-dim-100{opacity:1}.wp-block-post-featured-image:where(.alignleft,.alignright){width:100%}</style>
<style id="wp-block-post-title-inline-css">.wp-block-post-title{box-sizing:border-box;word-break:break-word}.wp-block-post-title a{display:inline-block}</style>
<style id="wp-block-post-date-inline-css">.wp-block-post-date{box-sizing:border-box}</style>
<style id="wp-block-paragraph-inline-css">.is-small-text{font-size:.875em}.is-regular-text{font-size:1em}.is-large-text{font-size:2.25em}.is-larger-text{font-size:3em}.has-drop-cap:not(:focus):first-letter{float:left;font-size:8.4em;font-style:normal;font-weight:100;line-height:.68;margin:.05em .1em 0 0;text-transform:uppercase}body.rtl .has-drop-cap:not(:focus):first-letter{float:none;margin-left:.1em}p.has-drop-cap.has-background{overflow:hidden}p.has-background{padding:1.25em 2.375em}:where(p.has-text-color:not(.has-link-color)) a{color:inherit}p.has-text-align-left[style*="writing-mode:vertical-lr"],p.has-text-align-right[style*="writing-mode:vertical-rl"]{rotate:180deg}</style>
<style id="wp-block-post-terms-inline-css">.wp-block-post-terms{box-sizing:border-box}.wp-block-post-terms .wp-block-post-terms__separator{white-space:pre-wrap}

				.is-style-pill a,
				.is-style-pill span:not([class], [data-rich-text-placeholder]) {
					display: inline-block;
					background-color: var(--wp--preset--color--base-2);
					padding: 0.375rem 0.875rem;
					border-radius: var(--wp--preset--spacing--20);
				}

				.is-style-pill a:hover {
					background-color: var(--wp--preset--color--contrast-3);
				}</style>
<style id="wp-block-list-inline-css">ol,ul{box-sizing:border-box}ol.has-background,ul.has-background{padding:1.25em 2.375em}

				ul.is-style-checkmark-list {
					list-style-type: "\2713";
				}

				ul.is-style-checkmark-list li {
					padding-inline-start: 1ch;
				}</style>
<style id="wp-block-spacer-inline-css">.wp-block-spacer{clear:both}</style>
<style id="wp-block-separator-inline-css">@charset "UTF-8";.wp-block-separator{border:none;border-top:2px solid}.wp-block-separator.is-style-dots{background:none!important;border:none;height:auto;line-height:1;text-align:center}.wp-block-separator.is-style-dots:before{color:currentColor;content:"¬∑¬∑¬∑";font-family:serif;font-size:1.5em;letter-spacing:2em;padding-left:2em}</style>
<style id="wp-block-heading-inline-css">h1.has-background,h2.has-background,h3.has-background,h4.has-background,h5.has-background,h6.has-background{padding:1.25em 2.375em}h1.has-text-align-left[style*=writing-mode]:where([style*=vertical-lr]),h1.has-text-align-right[style*=writing-mode]:where([style*=vertical-rl]),h2.has-text-align-left[style*=writing-mode]:where([style*=vertical-lr]),h2.has-text-align-right[style*=writing-mode]:where([style*=vertical-rl]),h3.has-text-align-left[style*=writing-mode]:where([style*=vertical-lr]),h3.has-text-align-right[style*=writing-mode]:where([style*=vertical-rl]),h4.has-text-align-left[style*=writing-mode]:where([style*=vertical-lr]),h4.has-text-align-right[style*=writing-mode]:where([style*=vertical-rl]),h5.has-text-align-left[style*=writing-mode]:where([style*=vertical-lr]),h5.has-text-align-right[style*=writing-mode]:where([style*=vertical-rl]),h6.has-text-align-left[style*=writing-mode]:where([style*=vertical-lr]),h6.has-text-align-right[style*=writing-mode]:where([style*=vertical-rl]){rotate:180deg}

				.is-style-asterisk:before {
					content: '';
					width: 1.5rem;
					height: 3rem;
					background: var(--wp--preset--color--contrast-2, currentColor);
					clip-path: path('M11.93.684v8.039l5.633-5.633 1.216 1.23-5.66 5.66h8.04v1.737H13.2l5.701 5.701-1.23 1.23-5.742-5.742V21h-1.737v-8.094l-5.77 5.77-1.23-1.217 5.743-5.742H.842V9.98h8.162l-5.701-5.7 1.23-1.231 5.66 5.66V.684h1.737Z');
					display: block;
				}

				/* Hide the asterisk if the heading has no content, to avoid using empty headings to display the asterisk only, which is an A11Y issue */
				.is-style-asterisk:empty:before {
					content: none;
				}

				.is-style-asterisk:-moz-only-whitespace:before {
					content: none;
				}

				.is-style-asterisk.has-text-align-center:before {
					margin: 0 auto;
				}

				.is-style-asterisk.has-text-align-right:before {
					margin-left: auto;
				}

				.rtl .is-style-asterisk.has-text-align-left:before {
					margin-right: auto;
				}</style>
<style id="wp-block-comment-template-inline-css">.wp-block-comment-template{box-sizing:border-box;list-style:none;margin-bottom:0;max-width:100%;padding:0}.wp-block-comment-template li{clear:both}.wp-block-comment-template ol{list-style:none;margin-bottom:0;max-width:100%;padding-left:2rem}.wp-block-comment-template.alignleft{float:left}.wp-block-comment-template.aligncenter{margin-left:auto;margin-right:auto;width:-moz-fit-content;width:fit-content}.wp-block-comment-template.alignright{float:right}</style>
<style id="wp-block-comments-pagination-inline-css">.wp-block-comments-pagination>.wp-block-comments-pagination-next,.wp-block-comments-pagination>.wp-block-comments-pagination-numbers,.wp-block-comments-pagination>.wp-block-comments-pagination-previous{margin-bottom:.5em;margin-right:.5em}.wp-block-comments-pagination>.wp-block-comments-pagination-next:last-child,.wp-block-comments-pagination>.wp-block-comments-pagination-numbers:last-child,.wp-block-comments-pagination>.wp-block-comments-pagination-previous:last-child{margin-right:0}.wp-block-comments-pagination .wp-block-comments-pagination-previous-arrow{display:inline-block;margin-right:1ch}.wp-block-comments-pagination .wp-block-comments-pagination-previous-arrow:not(.is-arrow-chevron){transform:scaleX(1)}.wp-block-comments-pagination .wp-block-comments-pagination-next-arrow{display:inline-block;margin-left:1ch}.wp-block-comments-pagination .wp-block-comments-pagination-next-arrow:not(.is-arrow-chevron){transform:scaleX(1)}.wp-block-comments-pagination.aligncenter{justify-content:center}</style>
<style id="wp-block-post-comments-form-inline-css">.wp-block-post-comments-form{box-sizing:border-box}.wp-block-post-comments-form[style*=font-weight] :where(.comment-reply-title){font-weight:inherit}.wp-block-post-comments-form[style*=font-family] :where(.comment-reply-title){font-family:inherit}.wp-block-post-comments-form[class*=-font-size] :where(.comment-reply-title),.wp-block-post-comments-form[style*=font-size] :where(.comment-reply-title){font-size:inherit}.wp-block-post-comments-form[style*=line-height] :where(.comment-reply-title){line-height:inherit}.wp-block-post-comments-form[style*=font-style] :where(.comment-reply-title){font-style:inherit}.wp-block-post-comments-form[style*=letter-spacing] :where(.comment-reply-title){letter-spacing:inherit}.wp-block-post-comments-form input[type=submit]{box-shadow:none;cursor:pointer;display:inline-block;overflow-wrap:break-word;text-align:center}.wp-block-post-comments-form input:not([type=submit]),.wp-block-post-comments-form textarea{border:1px solid #949494;font-family:inherit;font-size:1em}.wp-block-post-comments-form input:not([type=submit]):not([type=checkbox]),.wp-block-post-comments-form textarea{padding:calc(.667em + 2px)}.wp-block-post-comments-form .comment-form input:not([type=submit]):not([type=checkbox]):not([type=hidden]),.wp-block-post-comments-form .comment-form textarea{box-sizing:border-box;display:block;width:100%}.wp-block-post-comments-form .comment-form-author label,.wp-block-post-comments-form .comment-form-email label,.wp-block-post-comments-form .comment-form-url label{display:block;margin-bottom:.25em}.wp-block-post-comments-form .comment-form-cookies-consent{display:flex;gap:.25em}.wp-block-post-comments-form .comment-form-cookies-consent #wp-comment-cookies-consent{margin-top:.35em}.wp-block-post-comments-form .comment-reply-title{margin-bottom:0}.wp-block-post-comments-form .comment-reply-title :where(small){font-size:var(--wp--preset--font-size--medium,smaller);margin-left:.5em}</style>
<style id="wp-block-buttons-inline-css">.wp-block-buttons.is-vertical{flex-direction:column}.wp-block-buttons.is-vertical>.wp-block-button:last-child{margin-bottom:0}.wp-block-buttons>.wp-block-button{display:inline-block;margin:0}.wp-block-buttons.is-content-justification-left{justify-content:flex-start}.wp-block-buttons.is-content-justification-left.is-vertical{align-items:flex-start}.wp-block-buttons.is-content-justification-center{justify-content:center}.wp-block-buttons.is-content-justification-center.is-vertical{align-items:center}.wp-block-buttons.is-content-justification-right{justify-content:flex-end}.wp-block-buttons.is-content-justification-right.is-vertical{align-items:flex-end}.wp-block-buttons.is-content-justification-space-between{justify-content:space-between}.wp-block-buttons.aligncenter{text-align:center}.wp-block-buttons:not(.is-content-justification-space-between,.is-content-justification-right,.is-content-justification-left,.is-content-justification-center) .wp-block-button.aligncenter{margin-left:auto;margin-right:auto;width:100%}.wp-block-buttons[style*=text-decoration] .wp-block-button,.wp-block-buttons[style*=text-decoration] .wp-block-button__link{text-decoration:inherit}.wp-block-buttons.has-custom-font-size .wp-block-button__link{font-size:inherit}.wp-block-button.aligncenter{text-align:center}</style>
<style id="wp-block-button-inline-css">.wp-block-button__link{box-sizing:border-box;cursor:pointer;display:inline-block;text-align:center;word-break:break-word}.wp-block-button__link.aligncenter{text-align:center}.wp-block-button__link.alignright{text-align:right}:where(.wp-block-button__link){border-radius:9999px;box-shadow:none;padding:calc(.667em + 2px) calc(1.333em + 2px);text-decoration:none}.wp-block-button[style*=text-decoration] .wp-block-button__link{text-decoration:inherit}.wp-block-buttons>.wp-block-button.has-custom-width{max-width:none}.wp-block-buttons>.wp-block-button.has-custom-width .wp-block-button__link{width:100%}.wp-block-buttons>.wp-block-button.has-custom-font-size .wp-block-button__link{font-size:inherit}.wp-block-buttons>.wp-block-button.wp-block-button__width-25{width:calc(25% - var(--wp--style--block-gap, .5em)*.75)}.wp-block-buttons>.wp-block-button.wp-block-button__width-50{width:calc(50% - var(--wp--style--block-gap, .5em)*.5)}.wp-block-buttons>.wp-block-button.wp-block-button__width-75{width:calc(75% - var(--wp--style--block-gap, .5em)*.25)}.wp-block-buttons>.wp-block-button.wp-block-button__width-100{flex-basis:100%;width:100%}.wp-block-buttons.is-vertical>.wp-block-button.wp-block-button__width-25{width:25%}.wp-block-buttons.is-vertical>.wp-block-button.wp-block-button__width-50{width:50%}.wp-block-buttons.is-vertical>.wp-block-button.wp-block-button__width-75{width:75%}.wp-block-button.is-style-squared,.wp-block-button__link.wp-block-button.is-style-squared{border-radius:0}.wp-block-button.no-border-radius,.wp-block-button__link.no-border-radius{border-radius:0!important}.wp-block-button .wp-block-button__link:where(.is-style-outline),.wp-block-button:where(.is-style-outline)>.wp-block-button__link{border:2px solid;padding:.667em 1.333em}.wp-block-button .wp-block-button__link:where(.is-style-outline):not(.has-text-color),.wp-block-button:where(.is-style-outline)>.wp-block-button__link:not(.has-text-color){color:currentColor}.wp-block-button .wp-block-button__link:where(.is-style-outline):not(.has-background),.wp-block-button:where(.is-style-outline)>.wp-block-button__link:not(.has-background){background-color:initial;background-image:none}.wp-block-button .wp-block-button__link:where(.has-border-color){border-width:initial}.wp-block-button .wp-block-button__link:where([style*=border-top-color]){border-top-width:medium}.wp-block-button .wp-block-button__link:where([style*=border-right-color]){border-right-width:medium}.wp-block-button .wp-block-button__link:where([style*=border-bottom-color]){border-bottom-width:medium}.wp-block-button .wp-block-button__link:where([style*=border-left-color]){border-left-width:medium}.wp-block-button .wp-block-button__link:where([style*=border-style]){border-width:initial}.wp-block-button .wp-block-button__link:where([style*=border-top-style]){border-top-width:medium}.wp-block-button .wp-block-button__link:where([style*=border-right-style]){border-right-width:medium}.wp-block-button .wp-block-button__link:where([style*=border-bottom-style]){border-bottom-width:medium}.wp-block-button .wp-block-button__link:where([style*=border-left-style]){border-left-width:medium}</style>
<style id="wp-block-comments-inline-css">.wp-block-post-comments{box-sizing:border-box}.wp-block-post-comments .alignleft{float:left}.wp-block-post-comments .alignright{float:right}.wp-block-post-comments .navigation:after{clear:both;content:"";display:table}.wp-block-post-comments .commentlist{clear:both;list-style:none;margin:0;padding:0}.wp-block-post-comments .commentlist .comment{min-height:2.25em;padding-left:3.25em}.wp-block-post-comments .commentlist .comment p{font-size:1em;line-height:1.8;margin:1em 0}.wp-block-post-comments .commentlist .children{list-style:none;margin:0;padding:0}.wp-block-post-comments .comment-author{line-height:1.5}.wp-block-post-comments .comment-author .avatar{border-radius:1.5em;display:block;float:left;height:2.5em;margin-right:.75em;margin-top:.5em;width:2.5em}.wp-block-post-comments .comment-author cite{font-style:normal}.wp-block-post-comments .comment-meta{font-size:.875em;line-height:1.5}.wp-block-post-comments .comment-meta b{font-weight:400}.wp-block-post-comments .comment-meta .comment-awaiting-moderation{display:block;margin-bottom:1em;margin-top:1em}.wp-block-post-comments .comment-body .commentmetadata{font-size:.875em}.wp-block-post-comments .comment-form-author label,.wp-block-post-comments .comment-form-comment label,.wp-block-post-comments .comment-form-email label,.wp-block-post-comments .comment-form-url label{display:block;margin-bottom:.25em}.wp-block-post-comments .comment-form input:not([type=submit]):not([type=checkbox]),.wp-block-post-comments .comment-form textarea{box-sizing:border-box;display:block;width:100%}.wp-block-post-comments .comment-form-cookies-consent{display:flex;gap:.25em}.wp-block-post-comments .comment-form-cookies-consent #wp-comment-cookies-consent{margin-top:.35em}.wp-block-post-comments .comment-reply-title{margin-bottom:0}.wp-block-post-comments .comment-reply-title :where(small){font-size:var(--wp--preset--font-size--medium,smaller);margin-left:.5em}.wp-block-post-comments .reply{font-size:.875em;margin-bottom:1.4em}.wp-block-post-comments input:not([type=submit]),.wp-block-post-comments textarea{border:1px solid #949494;font-family:inherit;font-size:1em}.wp-block-post-comments input:not([type=submit]):not([type=checkbox]),.wp-block-post-comments textarea{padding:calc(.667em + 2px)}:where(.wp-block-post-comments input[type=submit]){border:none}</style>
<style id="wp-block-post-navigation-link-inline-css">.wp-block-post-navigation-link .wp-block-post-navigation-link__arrow-previous{display:inline-block;margin-right:1ch}.wp-block-post-navigation-link .wp-block-post-navigation-link__arrow-previous:not(.is-arrow-chevron){transform:scaleX(1)}.wp-block-post-navigation-link .wp-block-post-navigation-link__arrow-next{display:inline-block;margin-left:1ch}.wp-block-post-navigation-link .wp-block-post-navigation-link__arrow-next:not(.is-arrow-chevron){transform:scaleX(1)}.wp-block-post-navigation-link.has-text-align-left[style*="writing-mode: vertical-lr"],.wp-block-post-navigation-link.has-text-align-right[style*="writing-mode: vertical-rl"]{rotate:180deg}</style>
<style id="wp-emoji-styles-inline-css">img.wp-smiley, img.emoji {
		display: inline !important;
		border: none !important;
		box-shadow: none !important;
		height: 1em !important;
		width: 1em !important;
		margin: 0 0.07em !important;
		vertical-align: -0.1em !important;
		background: none !important;
		padding: 0 !important;
	}</style>
<style id="wp-block-library-inline-css">:root{--wp-admin-theme-color:#007cba;--wp-admin-theme-color--rgb:0,124,186;--wp-admin-theme-color-darker-10:#006ba1;--wp-admin-theme-color-darker-10--rgb:0,107,161;--wp-admin-theme-color-darker-20:#005a87;--wp-admin-theme-color-darker-20--rgb:0,90,135;--wp-admin-border-width-focus:2px;--wp-block-synced-color:#7a00df;--wp-block-synced-color--rgb:122,0,223;--wp-bound-block-color:#9747ff}@media (min-resolution:192dpi){:root{--wp-admin-border-width-focus:1.5px}}.wp-element-button{cursor:pointer}:root{--wp--preset--font-size--normal:16px;--wp--preset--font-size--huge:42px}:root .has-very-light-gray-background-color{background-color:#eee}:root .has-very-dark-gray-background-color{background-color:#313131}:root .has-very-light-gray-color{color:#eee}:root .has-very-dark-gray-color{color:#313131}:root .has-vivid-green-cyan-to-vivid-cyan-blue-gradient-background{background:linear-gradient(135deg,#00d084,#0693e3)}:root .has-purple-crush-gradient-background{background:linear-gradient(135deg,#34e2e4,#4721fb 50%,#ab1dfe)}:root .has-hazy-dawn-gradient-background{background:linear-gradient(135deg,#faaca8,#dad0ec)}:root .has-subdued-olive-gradient-background{background:linear-gradient(135deg,#fafae1,#67a671)}:root .has-atomic-cream-gradient-background{background:linear-gradient(135deg,#fdd79a,#004a59)}:root .has-nightshade-gradient-background{background:linear-gradient(135deg,#330968,#31cdcf)}:root .has-midnight-gradient-background{background:linear-gradient(135deg,#020381,#2874fc)}.has-regular-font-size{font-size:1em}.has-larger-font-size{font-size:2.625em}.has-normal-font-size{font-size:var(--wp--preset--font-size--normal)}.has-huge-font-size{font-size:var(--wp--preset--font-size--huge)}.has-text-align-center{text-align:center}.has-text-align-left{text-align:left}.has-text-align-right{text-align:right}#end-resizable-editor-section{display:none}.aligncenter{clear:both}.items-justified-left{justify-content:flex-start}.items-justified-center{justify-content:center}.items-justified-right{justify-content:flex-end}.items-justified-space-between{justify-content:space-between}.screen-reader-text{border:0;clip:rect(1px,1px,1px,1px);-webkit-clip-path:inset(50%);clip-path:inset(50%);height:1px;margin:-1px;overflow:hidden;padding:0;position:absolute;width:1px;word-wrap:normal!important}.screen-reader-text:focus{background-color:#ddd;clip:auto!important;-webkit-clip-path:none;clip-path:none;color:#444;display:block;font-size:1em;height:auto;left:5px;line-height:normal;padding:15px 23px 14px;text-decoration:none;top:5px;width:auto;z-index:100000}html :where(.has-border-color){border-style:solid}html :where([style*=border-top-color]){border-top-style:solid}html :where([style*=border-right-color]){border-right-style:solid}html :where([style*=border-bottom-color]){border-bottom-style:solid}html :where([style*=border-left-color]){border-left-style:solid}html :where([style*=border-width]){border-style:solid}html :where([style*=border-top-width]){border-top-style:solid}html :where([style*=border-right-width]){border-right-style:solid}html :where([style*=border-bottom-width]){border-bottom-style:solid}html :where([style*=border-left-width]){border-left-style:solid}html :where(img[class*=wp-image-]){height:auto;max-width:100%}:where(figure){margin:0 0 1em}html :where(.is-position-sticky){--wp-admin--admin-bar--position-offset:var(--wp-admin--admin-bar--height,0px)}@media screen and (max-width:600px){html :where(.is-position-sticky){--wp-admin--admin-bar--position-offset:0px}}</style>
<style id="global-styles-inline-css">body{--wp--preset--color--black: #000000;--wp--preset--color--cyan-bluish-gray: #abb8c3;--wp--preset--color--white: #ffffff;--wp--preset--color--pale-pink: #f78da7;--wp--preset--color--vivid-red: #cf2e2e;--wp--preset--color--luminous-vivid-orange: #ff6900;--wp--preset--color--luminous-vivid-amber: #fcb900;--wp--preset--color--light-green-cyan: #7bdcb5;--wp--preset--color--vivid-green-cyan: #00d084;--wp--preset--color--pale-cyan-blue: #8ed1fc;--wp--preset--color--vivid-cyan-blue: #0693e3;--wp--preset--color--vivid-purple: #9b51e0;--wp--preset--color--base: #D6D2CE;--wp--preset--color--base-2: #E1DFDB;--wp--preset--color--contrast: #1A1514;--wp--preset--color--contrast-2: #65574E;--wp--preset--color--contrast-3: #958D86;--wp--preset--gradient--vivid-cyan-blue-to-vivid-purple: linear-gradient(135deg,rgba(6,147,227,1) 0%,rgb(155,81,224) 100%);--wp--preset--gradient--light-green-cyan-to-vivid-green-cyan: linear-gradient(135deg,rgb(122,220,180) 0%,rgb(0,208,130) 100%);--wp--preset--gradient--luminous-vivid-amber-to-luminous-vivid-orange: linear-gradient(135deg,rgba(252,185,0,1) 0%,rgba(255,105,0,1) 100%);--wp--preset--gradient--luminous-vivid-orange-to-vivid-red: linear-gradient(135deg,rgba(255,105,0,1) 0%,rgb(207,46,46) 100%);--wp--preset--gradient--very-light-gray-to-cyan-bluish-gray: linear-gradient(135deg,rgb(238,238,238) 0%,rgb(169,184,195) 100%);--wp--preset--gradient--cool-to-warm-spectrum: linear-gradient(135deg,rgb(74,234,220) 0%,rgb(151,120,209) 20%,rgb(207,42,186) 40%,rgb(238,44,130) 60%,rgb(251,105,98) 80%,rgb(254,248,76) 100%);--wp--preset--gradient--blush-light-purple: linear-gradient(135deg,rgb(255,206,236) 0%,rgb(152,150,240) 100%);--wp--preset--gradient--blush-bordeaux: linear-gradient(135deg,rgb(254,205,165) 0%,rgb(254,45,45) 50%,rgb(107,0,62) 100%);--wp--preset--gradient--luminous-dusk: linear-gradient(135deg,rgb(255,203,112) 0%,rgb(199,81,192) 50%,rgb(65,88,208) 100%);--wp--preset--gradient--pale-ocean: linear-gradient(135deg,rgb(255,245,203) 0%,rgb(182,227,212) 50%,rgb(51,167,181) 100%);--wp--preset--gradient--electric-grass: linear-gradient(135deg,rgb(202,248,128) 0%,rgb(113,206,126) 100%);--wp--preset--gradient--midnight: linear-gradient(135deg,rgb(2,3,129) 0%,rgb(40,116,252) 100%);--wp--preset--gradient--gradient-1: linear-gradient(to bottom, #E1DFDB 0%, #D6D2CE 100%);--wp--preset--gradient--gradient-2: linear-gradient(to bottom, #958D86 0%, #D6D2CE 100%);--wp--preset--gradient--gradient-3: linear-gradient(to bottom, #65574E 0%, #D6D2CE 100%);--wp--preset--gradient--gradient-4: linear-gradient(to bottom, #1A1514 0%, #D6D2CE 100%);--wp--preset--gradient--gradient-5: linear-gradient(to bottom, #65574E 0%, #958D86 100%);--wp--preset--gradient--gradient-6: linear-gradient(to bottom, #1A1514 0%, #65574E 100%);--wp--preset--gradient--gradient-7: linear-gradient(to bottom, #D6D2CE 50%, #E1DFDB 50%);--wp--preset--gradient--gradient-8: linear-gradient(to bottom, #958D86 50%, #D6D2CE 50%);--wp--preset--gradient--gradient-9: linear-gradient(to bottom, #65574E 50%, #D6D2CE 50%);--wp--preset--gradient--gradient-10: linear-gradient(to bottom, #1A1514 50%, #D6D2CE 50%);--wp--preset--gradient--gradient-11: linear-gradient(to bottom, #65574E 50%, #958D86 50%);--wp--preset--gradient--gradient-12: linear-gradient(to bottom, #1A1514 50%, #65574E 50%);--wp--preset--font-size--small: 1rem;--wp--preset--font-size--medium: 1.2rem;--wp--preset--font-size--large: clamp(1.5rem, 1.5rem + ((1vw - 0.2rem) * 0.833), 2rem);--wp--preset--font-size--x-large: clamp(2rem, 2rem + ((1vw - 0.2rem) * 1.083), 2.65rem);--wp--preset--font-size--xx-large: clamp(2.65rem, 2.65rem + ((1vw - 0.2rem) * 1.417), 3.5rem);--wp--preset--font-family--heading: "Inter", sans-serif;--wp--preset--font-family--body: Cardo;--wp--preset--font-family--system-sans-serif: -apple-system, BlinkMacSystemFont, avenir next, avenir, segoe ui, helvetica neue, helvetica, Cantarell, Ubuntu, roboto, noto, arial, sans-serif;--wp--preset--font-family--system-serif: Iowan Old Style, Apple Garamond, Baskerville, Times New Roman, Droid Serif, Times, Source Serif Pro, serif, Apple Color Emoji, Segoe UI Emoji, Segoe UI Symbol;--wp--preset--spacing--10: 1rem;--wp--preset--spacing--20: min(1.5rem, 2vw);--wp--preset--spacing--30: min(2.5rem, 3vw);--wp--preset--spacing--40: min(4rem, 5vw);--wp--preset--spacing--50: min(6.5rem, 8vw);--wp--preset--spacing--60: min(10.5rem, 13vw);--wp--preset--shadow--natural: 6px 6px 9px rgba(0, 0, 0, 0.2);--wp--preset--shadow--deep: 12px 12px 50px rgba(0, 0, 0, 0.4);--wp--preset--shadow--sharp: 6px 6px 0px rgba(0, 0, 0, 0.2);--wp--preset--shadow--outlined: 6px 6px 0px -3px rgba(255, 255, 255, 1), 6px 6px rgba(0, 0, 0, 1);--wp--preset--shadow--crisp: 6px 6px 0px rgba(0, 0, 0, 1);}body { margin: 0;--wp--style--global--content-size: 620px;--wp--style--global--wide-size: 1280px; }.wp-site-blocks { padding-top: var(--wp--style--root--padding-top); padding-bottom: var(--wp--style--root--padding-bottom); }.has-global-padding { padding-right: var(--wp--style--root--padding-right); padding-left: var(--wp--style--root--padding-left); }.has-global-padding :where(.has-global-padding:not(.wp-block-block)) { padding-right: 0; padding-left: 0; }.has-global-padding > .alignfull { margin-right: calc(var(--wp--style--root--padding-right) * -1); margin-left: calc(var(--wp--style--root--padding-left) * -1); }.has-global-padding :where(.has-global-padding:not(.wp-block-block)) > .alignfull { margin-right: 0; margin-left: 0; }.has-global-padding > .alignfull:where(:not(.has-global-padding):not(.is-layout-flex):not(.is-layout-grid)) > :where([class*="wp-block-"]:not(.alignfull):not([class*="__"]),p,h1,h2,h3,h4,h5,h6,ul,ol) { padding-right: var(--wp--style--root--padding-right); padding-left: var(--wp--style--root--padding-left); }.has-global-padding :where(.has-global-padding) > .alignfull:where(:not(.has-global-padding)) > :where([class*="wp-block-"]:not(.alignfull):not([class*="__"]),p,h1,h2,h3,h4,h5,h6,ul,ol) { padding-right: 0; padding-left: 0; }.wp-site-blocks > .alignleft { float: left; margin-right: 2em; }.wp-site-blocks > .alignright { float: right; margin-left: 2em; }.wp-site-blocks > .aligncenter { justify-content: center; margin-left: auto; margin-right: auto; }:where(.wp-site-blocks) > * { margin-block-start: 1.2rem; margin-block-end: 0; }:where(.wp-site-blocks) > :first-child:first-child { margin-block-start: 0; }:where(.wp-site-blocks) > :last-child:last-child { margin-block-end: 0; }body { --wp--style--block-gap: 1.2rem; }:where(body .is-layout-flow)  > :first-child:first-child{margin-block-start: 0;}:where(body .is-layout-flow)  > :last-child:last-child{margin-block-end: 0;}:where(body .is-layout-flow)  > *{margin-block-start: 1.2rem;margin-block-end: 0;}:where(body .is-layout-constrained)  > :first-child:first-child{margin-block-start: 0;}:where(body .is-layout-constrained)  > :last-child:last-child{margin-block-end: 0;}:where(body .is-layout-constrained)  > *{margin-block-start: 1.2rem;margin-block-end: 0;}:where(body .is-layout-flex) {gap: 1.2rem;}:where(body .is-layout-grid) {gap: 1.2rem;}body .is-layout-flow > .alignleft{float: left;margin-inline-start: 0;margin-inline-end: 2em;}body .is-layout-flow > .alignright{float: right;margin-inline-start: 2em;margin-inline-end: 0;}body .is-layout-flow > .aligncenter{margin-left: auto !important;margin-right: auto !important;}body .is-layout-constrained > .alignleft{float: left;margin-inline-start: 0;margin-inline-end: 2em;}body .is-layout-constrained > .alignright{float: right;margin-inline-start: 2em;margin-inline-end: 0;}body .is-layout-constrained > .aligncenter{margin-left: auto !important;margin-right: auto !important;}body .is-layout-constrained > :where(:not(.alignleft):not(.alignright):not(.alignfull)){max-width: var(--wp--style--global--content-size);margin-left: auto !important;margin-right: auto !important;}body .is-layout-constrained > .alignwide{max-width: var(--wp--style--global--wide-size);}body .is-layout-flex{display: flex;}body .is-layout-flex{flex-wrap: wrap;align-items: center;}body .is-layout-flex > *{margin: 0;}body .is-layout-grid{display: grid;}body .is-layout-grid > *{margin: 0;}body{background-color: var(--wp--preset--color--base);color: var(--wp--preset--color--contrast);font-family: var(--wp--preset--font-family--body);font-size: var(--wp--preset--font-size--medium);font-style: normal;font-weight: 400;line-height: 1.55;--wp--style--root--padding-top: 0px;--wp--style--root--padding-right: var(--wp--preset--spacing--50);--wp--style--root--padding-bottom: 0px;--wp--style--root--padding-left: var(--wp--preset--spacing--50);}a:where(:not(.wp-element-button)){color: var(--wp--preset--color--contrast);text-decoration: underline;}a:where(:not(.wp-element-button)):hover{text-decoration: none;}h1, h2, h3, h4, h5, h6{color: var(--wp--preset--color--contrast);font-family: var(--wp--preset--font-family--heading);font-weight: normal;letter-spacing: 0;line-height: 1.2;}h1{font-size: var(--wp--preset--font-size--xx-large);line-height: 1.15;}h2{font-size: var(--wp--preset--font-size--x-large);}h3{font-size: var(--wp--preset--font-size--large);}h4{font-size: clamp(1.1rem, 1.1rem + ((1vw - 0.2rem) * 0.767), 1.5rem);}h5{font-size: var(--wp--preset--font-size--medium);}h6{font-size: var(--wp--preset--font-size--small);}.wp-element-button, .wp-block-button__link{background-color: var(--wp--preset--color--contrast-2);border-radius: 100px;border-color: var(--wp--preset--color--contrast-2);border-width: 0;color: var(--wp--preset--color--white);font-family: var(--wp--preset--font-family--heading);font-size: var(--wp--preset--font-size--small);font-style: normal;font-weight: 500;line-height: inherit;padding-top: 0.9rem;padding-right: 2rem;padding-bottom: 0.9rem;padding-left: 2rem;text-decoration: none;}.wp-element-button:hover, .wp-block-button__link:hover{background-color: var(--wp--preset--color--contrast);border-color: var(--wp--preset--color--contrast-2);color: var(--wp--preset--color--base);}.wp-element-button:focus, .wp-block-button__link:focus{background-color: var(--wp--preset--color--contrast-2);border-color: var(--wp--preset--color--contrast-2);color: var(--wp--preset--color--base);outline-color: var(--wp--preset--color--contrast);outline-offset: 2px;}.wp-element-button:active, .wp-block-button__link:active{background-color: var(--wp--preset--color--contrast);color: var(--wp--preset--color--base);}.wp-element-caption, .wp-block-audio figcaption, .wp-block-embed figcaption, .wp-block-gallery figcaption, .wp-block-image figcaption, .wp-block-table figcaption, .wp-block-video figcaption{color: var(--wp--preset--color--contrast-2);font-family: var(--wp--preset--font-family--body);font-size: 0.8rem;}.has-black-color{color: var(--wp--preset--color--black) !important;}.has-cyan-bluish-gray-color{color: var(--wp--preset--color--cyan-bluish-gray) !important;}.has-white-color{color: var(--wp--preset--color--white) !important;}.has-pale-pink-color{color: var(--wp--preset--color--pale-pink) !important;}.has-vivid-red-color{color: var(--wp--preset--color--vivid-red) !important;}.has-luminous-vivid-orange-color{color: var(--wp--preset--color--luminous-vivid-orange) !important;}.has-luminous-vivid-amber-color{color: var(--wp--preset--color--luminous-vivid-amber) !important;}.has-light-green-cyan-color{color: var(--wp--preset--color--light-green-cyan) !important;}.has-vivid-green-cyan-color{color: var(--wp--preset--color--vivid-green-cyan) !important;}.has-pale-cyan-blue-color{color: var(--wp--preset--color--pale-cyan-blue) !important;}.has-vivid-cyan-blue-color{color: var(--wp--preset--color--vivid-cyan-blue) !important;}.has-vivid-purple-color{color: var(--wp--preset--color--vivid-purple) !important;}.has-base-color{color: var(--wp--preset--color--base) !important;}.has-base-2-color{color: var(--wp--preset--color--base-2) !important;}.has-contrast-color{color: var(--wp--preset--color--contrast) !important;}.has-contrast-2-color{color: var(--wp--preset--color--contrast-2) !important;}.has-contrast-3-color{color: var(--wp--preset--color--contrast-3) !important;}.has-black-background-color{background-color: var(--wp--preset--color--black) !important;}.has-cyan-bluish-gray-background-color{background-color: var(--wp--preset--color--cyan-bluish-gray) !important;}.has-white-background-color{background-color: var(--wp--preset--color--white) !important;}.has-pale-pink-background-color{background-color: var(--wp--preset--color--pale-pink) !important;}.has-vivid-red-background-color{background-color: var(--wp--preset--color--vivid-red) !important;}.has-luminous-vivid-orange-background-color{background-color: var(--wp--preset--color--luminous-vivid-orange) !important;}.has-luminous-vivid-amber-background-color{background-color: var(--wp--preset--color--luminous-vivid-amber) !important;}.has-light-green-cyan-background-color{background-color: var(--wp--preset--color--light-green-cyan) !important;}.has-vivid-green-cyan-background-color{background-color: var(--wp--preset--color--vivid-green-cyan) !important;}.has-pale-cyan-blue-background-color{background-color: var(--wp--preset--color--pale-cyan-blue) !important;}.has-vivid-cyan-blue-background-color{background-color: var(--wp--preset--color--vivid-cyan-blue) !important;}.has-vivid-purple-background-color{background-color: var(--wp--preset--color--vivid-purple) !important;}.has-base-background-color{background-color: var(--wp--preset--color--base) !important;}.has-base-2-background-color{background-color: var(--wp--preset--color--base-2) !important;}.has-contrast-background-color{background-color: var(--wp--preset--color--contrast) !important;}.has-contrast-2-background-color{background-color: var(--wp--preset--color--contrast-2) !important;}.has-contrast-3-background-color{background-color: var(--wp--preset--color--contrast-3) !important;}.has-black-border-color{border-color: var(--wp--preset--color--black) !important;}.has-cyan-bluish-gray-border-color{border-color: var(--wp--preset--color--cyan-bluish-gray) !important;}.has-white-border-color{border-color: var(--wp--preset--color--white) !important;}.has-pale-pink-border-color{border-color: var(--wp--preset--color--pale-pink) !important;}.has-vivid-red-border-color{border-color: var(--wp--preset--color--vivid-red) !important;}.has-luminous-vivid-orange-border-color{border-color: var(--wp--preset--color--luminous-vivid-orange) !important;}.has-luminous-vivid-amber-border-color{border-color: var(--wp--preset--color--luminous-vivid-amber) !important;}.has-light-green-cyan-border-color{border-color: var(--wp--preset--color--light-green-cyan) !important;}.has-vivid-green-cyan-border-color{border-color: var(--wp--preset--color--vivid-green-cyan) !important;}.has-pale-cyan-blue-border-color{border-color: var(--wp--preset--color--pale-cyan-blue) !important;}.has-vivid-cyan-blue-border-color{border-color: var(--wp--preset--color--vivid-cyan-blue) !important;}.has-vivid-purple-border-color{border-color: var(--wp--preset--color--vivid-purple) !important;}.has-base-border-color{border-color: var(--wp--preset--color--base) !important;}.has-base-2-border-color{border-color: var(--wp--preset--color--base-2) !important;}.has-contrast-border-color{border-color: var(--wp--preset--color--contrast) !important;}.has-contrast-2-border-color{border-color: var(--wp--preset--color--contrast-2) !important;}.has-contrast-3-border-color{border-color: var(--wp--preset--color--contrast-3) !important;}.has-vivid-cyan-blue-to-vivid-purple-gradient-background{background: var(--wp--preset--gradient--vivid-cyan-blue-to-vivid-purple) !important;}.has-light-green-cyan-to-vivid-green-cyan-gradient-background{background: var(--wp--preset--gradient--light-green-cyan-to-vivid-green-cyan) !important;}.has-luminous-vivid-amber-to-luminous-vivid-orange-gradient-background{background: var(--wp--preset--gradient--luminous-vivid-amber-to-luminous-vivid-orange) !important;}.has-luminous-vivid-orange-to-vivid-red-gradient-background{background: var(--wp--preset--gradient--luminous-vivid-orange-to-vivid-red) !important;}.has-very-light-gray-to-cyan-bluish-gray-gradient-background{background: var(--wp--preset--gradient--very-light-gray-to-cyan-bluish-gray) !important;}.has-cool-to-warm-spectrum-gradient-background{background: var(--wp--preset--gradient--cool-to-warm-spectrum) !important;}.has-blush-light-purple-gradient-background{background: var(--wp--preset--gradient--blush-light-purple) !important;}.has-blush-bordeaux-gradient-background{background: var(--wp--preset--gradient--blush-bordeaux) !important;}.has-luminous-dusk-gradient-background{background: var(--wp--preset--gradient--luminous-dusk) !important;}.has-pale-ocean-gradient-background{background: var(--wp--preset--gradient--pale-ocean) !important;}.has-electric-grass-gradient-background{background: var(--wp--preset--gradient--electric-grass) !important;}.has-midnight-gradient-background{background: var(--wp--preset--gradient--midnight) !important;}.has-gradient-1-gradient-background{background: var(--wp--preset--gradient--gradient-1) !important;}.has-gradient-2-gradient-background{background: var(--wp--preset--gradient--gradient-2) !important;}.has-gradient-3-gradient-background{background: var(--wp--preset--gradient--gradient-3) !important;}.has-gradient-4-gradient-background{background: var(--wp--preset--gradient--gradient-4) !important;}.has-gradient-5-gradient-background{background: var(--wp--preset--gradient--gradient-5) !important;}.has-gradient-6-gradient-background{background: var(--wp--preset--gradient--gradient-6) !important;}.has-gradient-7-gradient-background{background: var(--wp--preset--gradient--gradient-7) !important;}.has-gradient-8-gradient-background{background: var(--wp--preset--gradient--gradient-8) !important;}.has-gradient-9-gradient-background{background: var(--wp--preset--gradient--gradient-9) !important;}.has-gradient-10-gradient-background{background: var(--wp--preset--gradient--gradient-10) !important;}.has-gradient-11-gradient-background{background: var(--wp--preset--gradient--gradient-11) !important;}.has-gradient-12-gradient-background{background: var(--wp--preset--gradient--gradient-12) !important;}.has-small-font-size{font-size: var(--wp--preset--font-size--small) !important;}.has-medium-font-size{font-size: var(--wp--preset--font-size--medium) !important;}.has-large-font-size{font-size: var(--wp--preset--font-size--large) !important;}.has-x-large-font-size{font-size: var(--wp--preset--font-size--x-large) !important;}.has-xx-large-font-size{font-size: var(--wp--preset--font-size--xx-large) !important;}.has-heading-font-family{font-family: var(--wp--preset--font-family--heading) !important;}.has-body-font-family{font-family: var(--wp--preset--font-family--body) !important;}.has-system-sans-serif-font-family{font-family: var(--wp--preset--font-family--system-sans-serif) !important;}.has-system-serif-font-family{font-family: var(--wp--preset--font-family--system-serif) !important;}
.wp-block-navigation{font-weight: 500;}
.wp-block-navigation a:where(:not(.wp-element-button)){color: inherit;text-decoration: none;}
.wp-block-navigation a:where(:not(.wp-element-button)):hover{text-decoration: underline;}
.wp-block-button.is-style-outline .wp-block-button__link{border-width: 2px;padding-top: calc(0.9rem - 2px);padding-right: calc(2rem - 2px);padding-bottom: calc(0.9rem - 2px);padding-left: calc(2rem - 2px);}
.wp-block-buttons-is-layout-flow > :first-child:first-child{margin-block-start: 0;}.wp-block-buttons-is-layout-flow > :last-child:last-child{margin-block-end: 0;}.wp-block-buttons-is-layout-flow > *{margin-block-start: 0.7rem;margin-block-end: 0;}.wp-block-buttons-is-layout-constrained > :first-child:first-child{margin-block-start: 0;}.wp-block-buttons-is-layout-constrained > :last-child:last-child{margin-block-end: 0;}.wp-block-buttons-is-layout-constrained > *{margin-block-start: 0.7rem;margin-block-end: 0;}.wp-block-buttons-is-layout-flex{gap: 0.7rem;}.wp-block-buttons-is-layout-grid{gap: 0.7rem;}
.wp-block-comments-pagination{font-size: var(--wp--preset--font-size--small);}
.wp-block-comments-pagination-next{font-size: var(--wp--preset--font-size--small);}
.wp-block-comments-pagination-previous{font-size: var(--wp--preset--font-size--small);}
ol,ul{padding-left: var(--wp--preset--spacing--10);}
.wp-block-post-author-name{font-size: var(--wp--preset--font-size--small);}
.wp-block-post-author-name a:where(:not(.wp-element-button)){text-decoration: none;}
.wp-block-post-author-name a:where(:not(.wp-element-button)):hover{text-decoration: underline;}
.wp-block-post-date{color: var(--wp--preset--color--contrast-2);font-size: var(--wp--preset--font-size--small);}
.wp-block-post-date a:where(:not(.wp-element-button)){color: var(--wp--preset--color--contrast-2);text-decoration: none;}
.wp-block-post-date a:where(:not(.wp-element-button)):hover{text-decoration: underline;}
.wp-block-post-featured-image img, .wp-block-post-featured-image .block-editor-media-placeholder, .wp-block-post-featured-image .wp-block-post-featured-image__overlay{border-radius: var(--wp--preset--spacing--20);}
.wp-block-post-terms{font-size: var(--wp--preset--font-size--small);}
.wp-block-post-terms a:where(:not(.wp-element-button)){text-decoration: none;}
.wp-block-post-terms a:where(:not(.wp-element-button)):hover{text-decoration: underline;}
.wp-block-post-title a:where(:not(.wp-element-button)){text-decoration: none;}
.wp-block-post-title a:where(:not(.wp-element-button)):hover{text-decoration: underline;}
.wp-block-separator{border-color: currentColor;border-width: 0 0 1px 0;border-style: solid;color: var(--wp--preset--color--contrast);}
.wp-block-site-title{font-family: var(--wp--preset--font-family--body);font-size: clamp(0.875rem, 0.875rem + ((1vw - 0.2rem) * 0.542), 1.2rem);font-style: normal;font-weight: 400;}
.wp-block-site-title a:where(:not(.wp-element-button)){text-decoration: none;}
.wp-block-site-title a:where(:not(.wp-element-button)):hover{text-decoration: none;}
:where(.wp-site-blocks *:focus){outline-width:2px;outline-style:solid}.wp-block-calendar.wp-block-calendar table:where(:not(.has-text-color)) th{background-color:var(--wp--preset--color--contrast-2);color:var(--wp--preset--color--base);border-color:var(--wp--preset--color--contrast-2)}.wp-block-calendar table:where(:not(.has-text-color)) td{border-color:var(--wp--preset--color--contrast-2)}.wp-block-categories{}.wp-block-categories{list-style-type:none;}.wp-block-categories li{margin-bottom: 0.5rem;}.wp-block-post-comments-form{}.wp-block-post-comments-form textarea, .wp-block-post-comments-form input{border-radius:.33rem}.wp-block-loginout{}.wp-block-loginout input{border-radius:.33rem;padding:calc(0.667em + 2px);border:1px solid #949494;}.wp-block-post-terms{}.wp-block-post-terms .wp-block-post-terms__prefix{color: var(--wp--preset--color--contrast-2);}.wp-block-query-title{}.wp-block-query-title span{font-style: italic;}.wp-block-quote{}.wp-block-quote :where(p){margin-block-start:0;margin-block-end:calc(var(--wp--preset--spacing--10) + 0.5rem);}.wp-block-quote :where(:last-child){margin-block-end:0;}.wp-block-quote.has-text-align-right.is-style-plain, .rtl .is-style-plain.wp-block-quote:not(.has-text-align-center):not(.has-text-align-left){border-width: 0 2px 0 0;padding-left:calc(var(--wp--preset--spacing--20) + 0.5rem);padding-right:calc(var(--wp--preset--spacing--20) + 0.5rem);}.wp-block-quote.has-text-align-left.is-style-plain, body:not(.rtl) .is-style-plain.wp-block-quote:not(.has-text-align-center):not(.has-text-align-right){border-width: 0 0 0 2px;padding-left:calc(var(--wp--preset--spacing--20) + 0.5rem);padding-right:calc(var(--wp--preset--spacing--20) + 0.5rem)}.wp-block-search{}.wp-block-search .wp-block-search__input{border-radius:.33rem}.wp-block-separator{}.wp-block-separator:not(.is-style-wide):not(.is-style-dots):not(.alignwide):not(.alignfull){width: var(--wp--preset--spacing--60)}</style>
<style id="core-block-supports-inline-css">.wp-container-core-group-is-layout-1.wp-container-core-group-is-layout-1 > *{margin-block-start:0;margin-block-end:0;}.wp-container-core-group-is-layout-1.wp-container-core-group-is-layout-1.wp-container-core-group-is-layout-1.wp-container-core-group-is-layout-1 > * + *{margin-block-start:0px;margin-block-end:0;}.wp-container-core-group-is-layout-2.wp-container-core-group-is-layout-2{gap:var(--wp--preset--spacing--20);}.wp-container-core-navigation-is-layout-1.wp-container-core-navigation-is-layout-1{gap:var(--wp--preset--spacing--20);justify-content:flex-end;}.wp-container-core-group-is-layout-3.wp-container-core-group-is-layout-3{justify-content:flex-start;}.wp-container-core-group-is-layout-4.wp-container-core-group-is-layout-4{justify-content:space-between;}.wp-container-core-group-is-layout-6.wp-container-core-group-is-layout-6{gap:0.3em;justify-content:flex-start;}.wp-container-core-group-is-layout-8.wp-container-core-group-is-layout-8{gap:var(--wp--preset--spacing--10);flex-direction:column;align-items:stretch;}.wp-container-core-group-is-layout-10.wp-container-core-group-is-layout-10{flex-wrap:nowrap;justify-content:space-between;}</style>
<style id="wp-block-template-skip-link-inline-css">.skip-link.screen-reader-text {
			border: 0;
			clip: rect(1px,1px,1px,1px);
			clip-path: inset(50%);
			height: 1px;
			margin: -1px;
			overflow: hidden;
			padding: 0;
			position: absolute !important;
			width: 1px;
			word-wrap: normal !important;
		}

		.skip-link.screen-reader-text:focus {
			background-color: #eee;
			clip: auto !important;
			clip-path: none;
			color: #444;
			display: block;
			font-size: 1em;
			height: auto;
			left: 5px;
			line-height: normal;
			padding: 15px 23px 14px;
			text-decoration: none;
			top: 5px;
			width: auto;
			z-index: 100000;
		}</style>
<link rel="stylesheet" id="tablepress-default-css" href="/blog/wp-content/tablepress-combined.min.css?ver=36" media="all">
<link rel="https://api.w.org/" href="/blog/wp-json/">
<link rel="alternate" type="application/json" href="/blog/wp-json/wp/v2/posts/2111">
<link rel="EditURI" type="application/rsd+xml" title="RSD" href="/blog/xmlrpc.php?rsd">
<meta name="generator" content="WordPress 6.5.3">
<link rel="canonical" href="/blog/bing-takes-a-look-to-the-idea-of-adaptive-attention/">
<link rel="shortlink" href="/blog/?p=2111">
<link rel="alternate" type="application/json+oembed" href="/blog/wp-json/oembed/1.0/embed?url=https%3A%2F%2F%2Fbing-takes-a-look-to-the-idea-of-adaptive-attention%2F">
<link rel="alternate" type="text/xml+oembed" href="/blog/wp-json/oembed/1.0/embed?url=https%3A%2F%2F%2Fbing-takes-a-look-to-the-idea-of-adaptive-attention%2F#038;format=xml">
<script id="wp-load-polyfill-importmap">
( HTMLScriptElement.supports && HTMLScriptElement.supports("importmap") ) || document.write( '<script src="https://a.rivero.nom.es/wp-includes/js/dist/vendor/wp-polyfill-importmap.min.js?ver=1.8.2"><\/scr' + 'ipt>' );
</script>
<script type="importmap" id="wp-importmap">{"imports":{"@wordpress\/interactivity":"\/blog\/wp-includes\/js\/dist\/interactivity.min.js?ver=6.5.3"}}</script>
<script type="module" src="/blog/wp-includes/blocks/navigation/view.min.js?ver=6.5.3" id="@wordpress/block-library/navigation-js-module"></script>
<link rel="modulepreload" href="/blog/wp-includes/js/dist/interactivity.min.js?ver=6.5.3" id="@wordpress/interactivity-js-modulepreload">
<style id="wp-fonts-local">@font-face{font-family:Inter;font-style:normal;font-weight:300 900;font-display:fallback;src:url('/blog/wp-content/themes/twentytwentyfour/assets/fonts/inter/Inter-VariableFont_slnt,wght.woff2') format('woff2');font-stretch:normal;}
@font-face{font-family:Cardo;font-style:normal;font-weight:400;font-display:fallback;src:url('/blog/wp-content/themes/twentytwentyfour/assets/fonts/cardo/cardo_normal_400.woff2') format('woff2');}
@font-face{font-family:Cardo;font-style:italic;font-weight:400;font-display:fallback;src:url('/blog/wp-content/themes/twentytwentyfour/assets/fonts/cardo/cardo_italic_400.woff2') format('woff2');}
@font-face{font-family:Cardo;font-style:normal;font-weight:700;font-display:fallback;src:url('/blog/wp-content/themes/twentytwentyfour/assets/fonts/cardo/cardo_normal_700.woff2') format('woff2');}</style>
</head>

<body class="post-template-default single single-post postid-2111 single-format-standard wp-embed-responsive">

<div class="wp-site-blocks">
<header class="wp-block-template-part">
<div class="wp-block-group alignwide has-base-background-color has-background has-global-padding is-layout-constrained wp-block-group-is-layout-constrained" style="padding-top:20px;padding-bottom:20px">
	
	<div class="wp-block-group alignwide is-content-justification-space-between is-layout-flex wp-container-core-group-is-layout-4 wp-block-group-is-layout-flex">
		
		<div class="wp-block-group is-layout-flex wp-container-core-group-is-layout-2 wp-block-group-is-layout-flex">
			

			
			<div class="wp-block-group is-layout-flow wp-container-core-group-is-layout-1 wp-block-group-is-layout-flow">
				<p class="wp-block-site-title"><a href="/blog/" target="_self" rel="home">Incidencias</a></p>
			</div>
			
		</div>
		

		
		<div class="wp-block-group is-content-justification-left is-layout-flex wp-container-core-group-is-layout-3 wp-block-group-is-layout-flex">
			<nav class="is-responsive items-justified-right wp-block-navigation is-horizontal is-content-justification-right is-layout-flex wp-container-core-navigation-is-layout-1 wp-block-navigation-is-layout-flex" aria-label="" data-wp-interactive="core/navigation" data-wp-context='{"overlayOpenedBy":{"click":false,"hover":false,"focus":false},"type":"overlay","roleAttribute":"","ariaLabel":"Men√∫"}'><button aria-haspopup="dialog" aria-label="Abrir el men√∫" class="wp-block-navigation__responsive-container-open " data-wp-on--click="actions.openMenuOnClick" data-wp-on--keydown="actions.handleMenuKeydown"><svg width="24" height="24" xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24" aria-hidden="true" focusable="false"><rect x="4" y="7.5" width="16" height="1.5"></rect><rect x="4" y="15" width="16" height="1.5"></rect></svg></button>
				<div class="wp-block-navigation__responsive-container  " id="modal-1" data-wp-class--has-modal-open="state.isMenuOpen" data-wp-class--is-menu-open="state.isMenuOpen" data-wp-watch="callbacks.initMenu" data-wp-on--keydown="actions.handleMenuKeydown" data-wp-on--focusout="actions.handleMenuFocusout" tabindex="-1">
					<div class="wp-block-navigation__responsive-close" tabindex="-1">
						<div class="wp-block-navigation__responsive-dialog" data-wp-bind--aria-modal="state.ariaModal" data-wp-bind--aria-label="state.ariaLabel" data-wp-bind--role="state.roleAttribute">
							<button aria-label="Cerrar el men√∫" class="wp-block-navigation__responsive-container-close" data-wp-on--click="actions.closeMenuOnClick"><svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24" width="24" height="24" aria-hidden="true" focusable="false"><path d="M13 11.8l6.1-6.3-1-1-6.1 6.2-6.1-6.2-1 1 6.1 6.3-6.5 6.7 1 1 6.5-6.6 6.5 6.6 1-1z"></path></svg></button>
							<div class="wp-block-navigation__responsive-container-content" data-wp-watch="callbacks.focusFirstElement" id="modal-1-content">
								<ul class="wp-block-navigation__container is-responsive items-justified-right wp-block-navigation"><ul class="wp-block-page-list">
<li class="wp-block-pages-list__item wp-block-navigation-item open-on-hover-click"><a class="wp-block-pages-list__item__link wp-block-navigation-item__content" href="https://web.archive.org/web/20071002115655/http://arivero.ciberpunk.net/">old blog until 2006</a></li>
<li class="wp-block-pages-list__item wp-block-navigation-item open-on-hover-click"><a class="wp-block-pages-list__item__link wp-block-navigation-item__content" href="/blog/research/">Old Research Page</a></li>
<li class="wp-block-pages-list__item wp-block-navigation-item open-on-hover-click"><a class="wp-block-pages-list__item__link wp-block-navigation-item__content" href="/blog/privacy-policy/">Privacy Policy</a></li>
</ul></ul>
							</div>
						</div>
					</div>
				</div></nav>
		</div>
		
	</div>
	
</div>
</header>


<main class="wp-block-group alignfull is-layout-flow wp-block-group-is-layout-flow">
<div class="wp-block-group has-global-padding is-layout-constrained wp-block-group-is-layout-constrained" style="margin-bottom:var(--wp--preset--spacing--40);padding-top:var(--wp--preset--spacing--50)">


<div class="wp-block-group is-vertical is-content-justification-stretch is-layout-flex wp-container-core-group-is-layout-8 wp-block-group-is-layout-flex" style="padding-top:0;padding-bottom:0">
<h1 class="wp-block-post-title has-x-large-font-size">Bing takes a look to the idea of adaptive attention.</h1>

<div class="wp-block-template-part">

<div class="wp-block-group has-global-padding is-layout-constrained wp-block-group-is-layout-constrained">
	
	<div class="wp-block-group is-content-justification-left is-layout-flex wp-container-core-group-is-layout-6 wp-block-group-is-layout-flex">
		<div class="wp-block-post-date"><time datetime="2023-04-29T18:11:56+02:00"><a href="/blog/bing-takes-a-look-to-the-idea-of-adaptive-attention/">Abr 29, 2023</a></time></div>

		
		<p class="has-contrast-2-color has-text-color">‚Äî</p>
		

		
		<p class="has-small-font-size has-contrast-2-color has-text-color">por</p>
		

		<div class="wp-block-post-author-name"><a href="/blog/author/arivero/" target="_self" class="wp-block-post-author-name__link">Alejandro Rivero</a></div>

		<div class="taxonomy-category wp-block-post-terms">
<span class="wp-block-post-terms__prefix">en </span><a href="/blog/category/generic/" rel="tag">General</a>
</div>

	</div>
	
</div>


</div>
</div>
</div>


<div class="entry-content alignfull wp-block-post-content has-global-padding is-layout-constrained wp-block-post-content-is-layout-constrained">
<p>Vamos a aprender juntos. Bing cuenta con tecnolog√≠a de IA, por lo que es posible que haya errores y sorpresas. Aseg√∫rese de comprobar los hechos y&nbsp;<a>compartir comentarios&nbsp;</a>para que podamos aprender y mejorar.</p>



<ul id="tone-options">
<li>M√°sCreativo</li>
</ul>



<p>Mensaje enviado. is attention a generalisation of kernel convolution?</p>



<p>Buscando: ‚Äò<code>attention kernel convolution</code>‚Äô</p>



<p>Buscando: ‚Äò<code>attention generalisation kernel convolution</code>‚Äô</p>



<p>Generando respuestas para usted‚Ä¶</p>



<p>That‚Äôs an interesting question. According to some papers I found, attention can be seen as a generalisation of kernel convolution in some cases.&nbsp;<a>For example, in&nbsp;</a><a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_Dynamic_Convolution_Attention_Over_Convolution_Kernels_CVPR_2020_paper.pdf" target="_blank" rel="noreferrer noopener"><sup>1</sup></a>, the authors propose&nbsp;<strong>dynamic convolution</strong>, which aggregates multiple parallel convolution kernels dynamically based on their attentions, which are input dependent. This can be seen as a generalisation of standard convolution, which uses a single kernel per layer.&nbsp;<a>Similarly, in&nbsp;</a><a href="https://www.sciencedirect.com/science/article/pii/S0925231222012796" target="_blank" rel="noreferrer noopener"><sup>2</sup></a>, the authors propose&nbsp;<strong>kernel attention convolution (KAConv)</strong>, which generates different attention weights for different spatial positions of convolution kernels based on the input features. This can be seen as a generalisation of standard convolution, which uses the same weight for each position of the kernel. However, these methods are not equivalent to attention mechanisms that operate on feature maps, such as self-attention or squeeze-and-excitation. Therefore, attention is not a generalisation of kernel convolution in general, but only in some specific cases.</p>



<p>M√°s informaci√≥n:</p>



<p><a target="_blank" href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_Dynamic_Convolution_Attention_Over_Convolution_Kernels_CVPR_2020_paper.pdf" rel="noreferrer noopener">1. openaccess.thecvf.com</a><a target="_blank" href="https://www.sciencedirect.com/science/article/pii/S0925231222012796" rel="noreferrer noopener">2. sciencedirect.com</a><a target="_blank" href="https://arxiv.org/abs/1912.03458" rel="noreferrer noopener">3. arxiv.org</a><a target="_blank" href="https://www.sciencedirect.com/science/article/abs/pii/S0925231222012796" rel="noreferrer noopener">4. sciencedirect.com</a><a target="_blank" href="https://arxiv.org/abs/2108.08205" rel="noreferrer noopener">5. arxiv.org</a>+2&nbsp;m√°s</p>



<p>1de20</p>



<p>Mensaje enviado. Can you show me some examples of attention mechanisms that operate on feature maps?</p>



<p>Buscando: ‚Äò<code>attention mechanisms feature maps</code>‚Äô</p>



<p>Generando respuestas para usted‚Ä¶</p>



<p>Sure, I can show you some examples. Attention mechanisms that operate on feature maps are methods that modify the feature maps based on some attention weights, which can be computed from the feature maps themselves or from other sources.&nbsp;<a>For example,&nbsp;<strong>self-attention</strong>&nbsp;is a method that computes the attention weights from the feature maps by using a query-key-value mechanism</a><a href="https://www.nature.com/articles/s41598-020-67529-x" target="_blank" rel="noreferrer noopener"><sup>1</sup></a>. This allows the network to capture long-range dependencies between different regions of the feature maps.&nbsp;<a>Another example is&nbsp;<strong>squeeze-and-excitation</strong>&nbsp;(SE), which computes the attention weights from the feature maps by using global average pooling and a fully connected layer</a><a href="https://www.mdpi.com/2076-3417/12/8/3846" target="_blank" rel="noreferrer noopener"><sup>2</sup></a>. This allows the network to recalibrate the channel-wise features by exploiting inter-channel dependencies.&nbsp;<a>A third example is&nbsp;<strong>non-local</strong>&nbsp;(NL), which computes the attention weights from the feature maps by using a pairwise function and a softmax function</a><a href="https://www.researchgate.net/publication/365625430_Enhanced_mechanisms_of_pooling_and_channel_attention_for_deep_learning_feature_maps" target="_blank" rel="noreferrer noopener"><sup>3</sup></a>. This allows the network to capture long-range dependencies between different pixels of the feature maps by using a similarity measure. These are some of the most popular attention mechanisms that operate on feature maps, but there are many others as well.</p>



<p>M√°s informaci√≥n:</p>



<p><a target="_blank" href="https://www.nature.com/articles/s41598-020-67529-x" rel="noreferrer noopener">1. nature.com</a><a target="_blank" href="https://www.mdpi.com/2076-3417/12/8/3846" rel="noreferrer noopener">2. mdpi.com</a><a target="_blank" href="https://www.researchgate.net/publication/365625430_Enhanced_mechanisms_of_pooling_and_channel_attention_for_deep_learning_feature_maps" rel="noreferrer noopener">3. researchgate.net</a></p>



<p>2de20https://www.bing.com/search?showselans=1&amp;IG=771026BE4B0542EA9AF63643621020B6&amp;IID=SERP.5027&amp;cw=1140&amp;ch=1278&amp;kseed=8500&amp;SFX=4&amp;q=attention+mechanisms+feature+maps&amp;iframeid=b844616f-5726-40b5-b8b9-8917cbe19123</p>



<p>Mensaje enviado. The feature maps include the position, do they?</p>



<p>Yes, the feature maps include the position information of the input image. The position information is encoded by the spatial location of the feature map pixels. For example, the top-left pixel of a feature map corresponds to the top-left region of the input image, and the bottom-right pixel of a feature map corresponds to the bottom-right region of the input image. The position information is important for tasks that require spatial reasoning, such as object detection or segmentation.</p>



<p>3de20</p>



<p>so consider a generic convolution kernel and show how you can code it using attention in the position part</p>



<p>Mensaje enviado. so consider a generic convolution kernel and show how you can code it using attention in the position part</p>



<p>Buscando: ‚Äò<code>convolution kernel attention position</code>‚Äô</p>



<p>Generando respuestas para usted‚Ä¶</p>



<p>I‚Äôm not sure if I understand your question correctly, but I‚Äôll try to answer it. A generic convolution kernel is a matrix of weights that is applied to a region of the input feature map to produce an output feature map. To code it using attention in the position part, we need to modify the kernel weights based on the position information of the input feature map.&nbsp;<a href="https://arxiv.org/abs/2211.07157" target="_blank" rel="noreferrer noopener">One possible way to do this is to use&nbsp;<strong>oversized convolution</strong>, which employs a kernel with twice the input size to model long-range dependencies through a global receptive field</a><a href="https://arxiv.org/abs/2211.07157" target="_blank" rel="noreferrer noopener"><sup>1</sup></a>. This also achieves implicit positional encoding by removing the shift-invariant property from convolution kernels, i.e., the effective kernels at different spatial locations are different when the kernel size is twice as large as the input size.&nbsp;<a href="https://www.sciencedirect.com/science/article/pii/S0925231222012796" target="_blank" rel="noreferrer noopener">Another possible way to do this is to use&nbsp;<strong>kernel attention convolution (KAConv)</strong>, which generates different attention weights for different spatial positions of convolution kernels based on the input features</a><a href="https://www.sciencedirect.com/science/article/pii/S0925231222012796" target="_blank" rel="noreferrer noopener"><sup>2</sup></a>. This allows the network to dynamically adjust the parameters of convolution kernels during the forward propagation to enhance the flexibility of convolution. These are some examples of how to code a generic convolution kernel using attention in the position part, but there may be other ways as well.</p>



<p>M√°s informaci√≥n:</p>



<p><a target="_blank" href="https://arxiv.org/abs/2211.07157" rel="noreferrer noopener">1. arxiv.org</a><a target="_blank" href="https://www.sciencedirect.com/science/article/pii/S0925231222012796" rel="noreferrer noopener">2. sciencedirect.com</a><a target="_blank" href="https://arxiv.org/abs/1912.03458" rel="noreferrer noopener">3. arxiv.org</a></p>



<p>4de20</p>



<p>Mensaje enviado. Can you show me some code snippets of how to implement these methods?</p>



<p>Buscando: ‚Äò<code>oversized convolution code</code>‚Äô</p>



<p>Generando respuestas para usted‚Ä¶</p>



<p>Sure, I can show you some code snippets of how to implement these methods. Here is a possible way to implement oversized convolution in PyTorch:</p>


<div class="wp-block-syntaxhighlighter-code "><pre class="brush: python; title: ; notranslate" title="">
import torch
import torch.nn as nn

class OversizedConv(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):
        super(OversizedConv, self).__init__()
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.kernel_size = kernel_size
        self.stride = stride
        self.padding = padding
        # initialize the oversized kernel with random weights
        self.weight = nn.Parameter(torch.randn(out_channels, in_channels, kernel_size * 2, kernel_size * 2))
        # initialize the bias with zeros
        self.bias = nn.Parameter(torch.zeros(out_channels))

    def forward(self, x):
        # get the input size
        batch_size, _, height, width = x.size()
        # pad the input with zeros to match the oversized kernel size
        x = nn.functional.pad(x, (self.kernel_size - 1, self.kernel_size - 1, self.kernel_size - 1, self.kernel_size - 1))
        # apply the convolution operation
        y = nn.functional.conv2d(x, self.weight, self.bias, self.stride, self.padding)
        # return the output feature map
        return y

</pre></div>


<p>Here is a possible way to implement kernel attention convolution in PyTorch:</p>


<div class="wp-block-syntaxhighlighter-code "><pre class="brush: python; title: ; notranslate" title="">
import torch
import torch.nn as nn

class KernelAttentionConv(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):
        super(KernelAttentionConv, self).__init__()
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.kernel_size = kernel_size
        self.stride = stride
        self.padding = padding
        # initialize the standard convolution kernel with random weights
        self.weight = nn.Parameter(torch.randn(out_channels, in_channels, kernel_size, kernel_size))
        # initialize the bias with zeros
        self.bias = nn.Parameter(torch.zeros(out_channels))
        # initialize the attention convolution kernel with random weights
        self.attention_weight = nn.Parameter(torch.randn(out_channels * kernel_size * kernel_size, in_channels * kernel_size * kernel_size))
        
    def forward(self, x):
        # get the input size
        batch_size, _, height, width = x.size()
        # apply the standard convolution operation
        y = nn.functional.conv2d(x, self.weight, self.bias, self.stride, self.padding)
        # get the output size
        _, _, height_out, width_out = y.size()
        # reshape the input feature map to a matrix of size (batch_size * height_out * width_out) x (in_channels * kernel_size * kernel_size)
        x_matrix = x.unfold(2, self.kernel_size, self.stride).unfold(3, self.kernel_size, self.stride).contiguous().view(batch_size * height_out * width_out,
                                                                                                                      -1)
        
        # apply the attention convolution operation to get the attention weights matrix of size (batch_size * height_out * width_out) x (out_channels * kernel_size * kernel_size)
        
        attention_matrix = torch.matmul(x_matrix,
                                        torch.t(self.attention_weight)).view(batch_size,
                                                                             height_out,
                                                                             width_out,
                                                                             -1)
        
        # apply softmax function to normalize the attention weights along the last dimension
        
        attention_matrix = nn.functional.softmax(attention_matrix,
                                                 dim=-1)
        
        # reshape the attention weights matrix to a tensor of size (batch_size) x (out_channels) x (kernel_size) x (kernel_size) x (height_out) x (width_out)
        
        attention_tensor = attention_matrix.view(batch_size,
                                                 out_channels,
                                                 kernel_size,
                                                 kernel_size,
                                                 height_out,
                                                 width_out)
        
        # reshape the standard convolution weight to a tensor of size (out_channels) x (in_channels) x (kernel_size) x (kernel_size) x 1 x 1
        
        weight_tensor = self.weight.view(out_channels,
                                         in_channels,
                                         kernel_size,
                                         kernel_size,
                                         1,
                                         1)
        
        # multiply the weight tensor and the attention tensor element-wise to get the refined weight tensor of size (batch_size) x (out_channels) x (in_channels) x (kernel_size) x (kernel_size) x (height_out) x (width_out
</pre></div>


<p>M√°s informaci√≥n:</p>



<p><a target="_blank" href="https://arxiv.org/abs/2211.07157" rel="noreferrer noopener">1. arxiv.org</a><a target="_blank" href="https://en.wikipedia.org/wiki/Convolutional_code" rel="noreferrer noopener">2. en.wikipedia.org</a><a target="_blank" href="http://export.arxiv.org/abs/2211.07157" rel="noreferrer noopener">3. export.arxiv.org</a></p>



<p>5de20</p>



<p>Mensaje enviado. Thank you for the code snippets.</p>



<p>You‚Äôre welcome. I‚Äôm glad I could help. ?</p>



<p>6de20</p>



<p>Mensaje enviado. could we conclude that these code proves that attention generalizes convolution?</p>



<p>I don‚Äôt think we can conclude that from these code snippets. These code snippets only show how to implement some specific methods that use attention to modify convolution kernels. They do not prove that attention generalizes convolution in general. In fact, as I mentioned before, attention is not a generalization of convolution in general, but only in some specific cases. There may be other ways to use attention that are not related to convolution at all, or other ways to use convolution that are not related to attention at all. Therefore, I think it is more accurate to say that attention and convolution are two different but related concepts that can be combined in various ways to achieve different effects.</p>



<p>7de20</p>



<p>Mensaje enviado. let me consider other kind of layers, the ones that reduce or increase the number of hidden neurons</p>



<p>OK, what do you want to know about those layers?</p>



<p>8de20</p>



<p>Mensaje enviado. first enumerate some examples</p>



<p>Some examples of layers that reduce or increase the number of hidden neurons are:</p>



<ul>
<li>
<strong>Pooling layers</strong>: These layers reduce the spatial size of the feature maps by applying a pooling operation, such as max or average, over a local region. This reduces the number of hidden neurons and also introduces some invariance to small translations. Pooling layers are often used between convolutional layers to reduce the computational cost and improve the generalization ability of the network.</li>



<li>
<strong>Fully connected layers</strong>: These layers connect every neuron in one layer to every neuron in another layer. This allows the network to learn non-linear combinations of the features from the previous layer. Fully connected layers can increase or decrease the number of hidden neurons depending on the output dimension. Fully connected layers are often used at the end of the network to perform classification or regression tasks.</li>



<li>
<strong>Bottleneck layers</strong>: These layers reduce the number of hidden neurons by applying a linear transformation, such as a matrix multiplication or a convolution with a 1&#215;1 kernel. This reduces the number of parameters and also compresses the information from the previous layer. Bottleneck layers are often used in residual blocks or dense blocks to improve the efficiency and performance of the network.</li>



<li>
<strong>Upsampling layers</strong>: These layers increase the spatial size of the feature maps by applying an upsampling operation, such as nearest neighbor or bilinear interpolation. This increases the number of hidden neurons and also restores some resolution from the previous layer. Upsampling layers are often used in decoder networks or generative networks to produce high-resolution outputs from low-resolution inputs.</li>
</ul>



<p>9de20</p>



<p>Mensaje enviado. a tricky think: could we implement pooling using attention?</p>



<p>Buscando: ‚Äò<code>pooling attention</code>‚Äô</p>



<p>Generando respuestas para usted‚Ä¶</p>



<p>That‚Äôs an interesting idea. I think we can implement pooling using attention in some ways.&nbsp;<a>For example, one way is to use&nbsp;<strong>attention pooling</strong>&nbsp;</a><a href="https://d2l.ai/chapter_attention-mechanisms-and-transformers/attention-pooling.html" target="_blank" rel="noreferrer noopener"><sup>1</sup></a>, which uses the same self-attention process with one significant modification. In a normal implementation, the output of the attention mechanism is the same dimension as the input. However, in attention pooling, the output dimension is reduced by a factor of r, where r is the pooling ratio. This can be achieved by using a linear projection layer to reduce the dimension of the queries and keys before applying the softmax function. This way, the attention weights are computed over a smaller set of queries and keys, resulting in a coarser representation of the input.&nbsp;<a>Another way is to use&nbsp;<strong>pooling attention</strong>&nbsp;</a><a href="https://arxiv.org/abs/2105.04371" target="_blank" rel="noreferrer noopener"><sup>2</sup></a>, which uses a pooling operation, such as max or average, over a local region of the input feature map to produce an output feature map. Then, an attention mechanism is applied to the output feature map to refine it based on some attention weights. These weights are computed from the input feature map by using a query-key-value mechanism. This way, the pooling operation reduces the spatial size of the feature map while preserving some information, and the attention mechanism enhances the representation by capturing long-range dependencies. These are some possible ways to implement pooling using attention, but there may be other ways as well.</p>



<p>M√°s informaci√≥n:</p>



<p><a target="_blank" href="https://d2l.ai/chapter_attention-mechanisms-and-transformers/attention-pooling.html" rel="noreferrer noopener">1. d2l.ai</a><a target="_blank" href="https://arxiv.org/abs/2105.04371" rel="noreferrer noopener">2. arxiv.org</a><a target="_blank" href="https://benjaminwarner.dev/2022/07/14/tinkering-with-attention-pooling" rel="noreferrer noopener">3. benjaminwarner.dev</a><a target="_blank" href="https://blog.csdn.net/jerry_liufeng/article/details/122898434" rel="noreferrer noopener">4. blog.csdn.net</a><a target="_blank" href="https://www.sciencedirect.com/science/article/abs/pii/S0020025516306673" rel="noreferrer noopener">5. sciencedirect.com</a>+1&nbsp;m√°s</p>



<p>10de20</p>



<p>Mensaje enviado. Can you show me some code snippets of how to implement these methods?</p>



<p>Buscando: ‚Äò<code>attention pooling code</code>‚Äô</p>



<p>Generando respuestas para usted‚Ä¶</p>



<p>Sure, I can show you some code snippets of how to implement these methods. Here is a possible way to implement attention pooling in PyTorch:</p>


<div class="wp-block-syntaxhighlighter-code "><pre class="brush: python; title: ; notranslate" title="">
import torch
import torch.nn as nn

class AttentionPooling(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):
        super(AttentionPooling, self).__init__()
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.kernel_size = kernel_size
        self.stride = stride
        self.padding = padding
        # initialize the standard convolution kernel with random weights
        self.weight = nn.Parameter(torch.randn(out_channels, in_channels, kernel_size, kernel_size))
        # initialize the bias with zeros
        self.bias = nn.Parameter(torch.zeros(out_channels))
        # initialize the linear projection layer to reduce the dimension of queries and keys by a factor of r
        self.r = 2 # pooling ratio
        self.projection = nn.Linear(in_channels * kernel_size * kernel_size, in_channels * kernel_size * kernel_size // self.r)

    def forward(self, x):
        # get the input size
        batch_size, _, height, width = x.size()
        # apply the standard convolution operation
        y = nn.functional.conv2d(x, self.weight, self.bias, self.stride, self.padding)
        # get the output size
        _, _, height_out, width_out = y.size()
        # reshape the input feature map to a matrix of size (batch_size * height_out * width_out) x (in_channels * kernel_size * kernel_size)
        x_matrix = x.unfold(2, self.kernel_size, self.stride).unfold(3, self.kernel_size, self.stride).contiguous().view(batch_size * height_out * width_out,
                                                                                                                      -1)
        
        # apply the linear projection layer to reduce the dimension of queries and keys by a factor of r
        
        q_matrix = k_matrix = self.projection(x_matrix)
        
        # apply the softmax function to compute the attention weights matrix of size (batch_size * height_out * width_out) x (batch_size * height_out * width_out)
        
        attention_matrix = nn.functional.softmax(torch.matmul(q_matrix,
                                                              torch.t(k_matrix)) / torch.sqrt(torch.tensor(self.in_channels * self.kernel_size * self.kernel_size // self.r)),
                                                 dim=-1)
        
        # reshape the output feature map to a matrix of size (batch_size * height_out * width_out) x (out_channels)
        
        y_matrix = y.view(batch_size * height_out * width_out,
                          -1)
        
        # apply the attention pooling operation to get the refined output feature map matrix of size (batch_size * height_out * width_out // r) x (out_channels)
        
        z_matrix = torch.matmul(attention_matrix&#x5B;:, ::self.r],
                                y_matrix)
        
        # reshape the refined output feature map matrix to a tensor of size (batch_size) x (out_channels) x (height_out // r) x (width_out // r)
        
        z = z_matrix.view(batch_size,
                          out_channels,
                          height_out // self.r,
                          width_out // self.r)
        
        # return the refined output feature map
        
        return z

</pre></div>


<p>Here is a possible way to implement pooling attention in PyTorch:</p>


<div class="wp-block-syntaxhighlighter-code "><pre class="brush: python; title: ; notranslate" title="">
import torch
import torch.nn as nn

class PoolingAttention(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):
        super(PoolingAttention, self).__init__()
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.kernel_size = kernel_size
        self.stride = stride
        self.padding = padding
        # initialize the standard convolution kernel with random weights
        self.weight = nn.Parameter(torch.randn(out_channels, in_channels, kernel_size, kernel_size))
        # initialize the bias with zeros
        self.bias = nn.Parameter(torch.zeros(out_channels))
        # initialize the query-key-value layers for computing attention weights
        self.query_layer = nn.Linear(in_channels * kernel_size * kernel_size,
                                     out_channels)
        
        self.key_layer = nn.Linear(in_channels * kernel_size * kernel_size,
                                   out_channels)
        
        self.value_layer = nn.Linear(in_channels * kernel_size * kernel_size,
                                     out_channels)

    def forward(self, x):
        # get the input size
        batch_size, _, height, width = x.size()
        
         # apply a pooling operation over
</pre></div>


<p>M√°s informaci√≥n:</p>



<p><a target="_blank" href="https://d2l.ai/chapter_attention-mechanisms-and-transformers/attention-pooling.html" rel="noreferrer noopener">1. d2l.ai</a><a target="_blank" href="https://github.com/inyeoplee77/SAGPool" rel="noreferrer noopener">2. github.com</a><a target="_blank" href="https://paperswithcode.com/method/spatial-attention-module" rel="noreferrer noopener">3. paperswithcode.com</a></p>



<p>11de20</p>



<p>Mensaje enviado. what about using attention for upsampling?</p>



<p>Buscando: ‚Äò<code>attention upsampling</code>‚Äô</p>



<p>Generando respuestas para usted‚Ä¶</p>



<p>Sure, I can show you some code snippets of how to implement these methods. Here is a possible way to implement attention upsampling in PyTorch:</p>


<div class="wp-block-syntaxhighlighter-code "><pre class="brush: python; title: ; notranslate" title="">
import torch
import torch.nn as nn

class AttentionUpsampling(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):
        super(AttentionUpsampling, self).__init__()
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.kernel_size = kernel_size
        self.stride = stride
        self.padding = padding
        # initialize the standard convolution kernel with random weights
        self.weight = nn.Parameter(torch.randn(out_channels, in_channels, kernel_size, kernel_size))
        # initialize the bias with zeros
        self.bias = nn.Parameter(torch.zeros(out_channels))
        # initialize the query-key-value layers for computing attention weights
        self.query_layer = nn.Linear(in_channels * kernel_size * kernel_size,
                                     out_channels)
        
        self.key_layer = nn.Linear(in_channels * kernel_size * kernel_size,
                                   out_channels)
        
        self.value_layer = nn.Linear(in_channels * kernel_size * kernel_size,
                                     out_channels)

    def forward(self, x):
        # get the input size
        batch_size, _, height, width = x.size()
        
         # apply an upsampling operation over a local region of the input feature map to produce an output feature map
        
        y = nn.functional.interpolate(x,
                                      scale_factor=self.stride,
                                      mode=&#039;bilinear&#039;,
                                      align_corners=False)
        
         # get the output size
        
        _, _, height_out, width_out = y.size()
        
         # reshape the input feature map to a matrix of size (batch_size * height_out * width_out) x (in_channels * kernel_size * kernel_size)
        
        x_matrix = x.unfold(2,
                            self.kernel_size,
                            1).unfold(3,
                                      self.kernel_size,
                                      1).contiguous().view(batch_size * height_out * width_out,
                                                           -1)
        
         # apply the query-key-value layers to get the queries, keys and values matrices of size (batch_size * height_out * width_out) x (out_channels)
        
        q_matrix = self.query_layer(x_matrix)
        
        k_matrix = self.key_layer(x_matrix)
        
        v_matrix = self.value_layer(x_matrix)
        
         # apply the softmax function to compute the attention weights matrix of size (batch_size * height_out * width_out) x (batch_size * height_out * width_out)
        
        attention_matrix = nn.functional.softmax(torch.matmul(q_matrix,
                                                              torch.t(k_matrix)) / torch.sqrt(torch.tensor(self.out_channels)),
                                                 dim=-1)
        
         # reshape the output feature map to a matrix of size (batch_size * height_out * width_out) x (out_channels)
        
        y_matrix = y.view(batch_size * height_out * width_out,
                          -1)
        
         # apply the attention upsampling operation to get the refined output feature map matrix of size (batch_size * height_out * width_out) x (out_channels)
        
        z_matrix = torch.matmul(attention_matrix,
                                v_matrix) + y_matrix
        
         # reshape the refined output feature map matrix to a tensor of size (batch_size) x (out_channels) x (height_out) x (width_out)
        
        z = z_matrix.view(batch_size,
                          out_channels,
                          height_out,
                          width_out)
        
         # return the refined output feature map
        
        return z

</pre></div>


<p>M√°s informaci√≥n:</p>



<p><a target="_blank" href="https://arxiv.org/abs/2012.09904" rel="noreferrer noopener">1. arxiv.org</a><a target="_blank" href="https://deepai.org/publication/attention-based-image-upsampling" rel="noreferrer noopener">2. deepai.org</a><a target="_blank" href="https://www.researchgate.net/publication/347515530_Attention-based_Image_Upsampling" rel="noreferrer noopener">3. researchgate.net</a></p>



<p>12de20</p>



<p>Mensaje enviado. Can you explain how these methods work in more detail?</p>



<p>Sure, I can explain how these methods work in more detail. The attention upsampling method works as follows:</p>



<ul>
<li>First, it applies an upsampling operation over a local region of the input feature map to produce an output feature map. This operation increases the spatial size of the feature map by a factor of stride, which is a hyperparameter. The upsampling operation can be any interpolation method, such as nearest neighbor or bilinear interpolation. This operation preserves some information from the input feature map, but also introduces some blur or aliasing effects.</li>



<li>Second, it applies an attention mechanism to the output feature map to refine it based on some attention weights. These weights are computed from the input feature map by using a query-key-value mechanism. The query-key-value mechanism works as follows:
<ul>
<li>It reshapes the input feature map to a matrix of size (batch_size * height_out * width_out) x (in_channels * kernel_size * kernel_size), where height_out and width_out are the output height and width, and kernel_size is a hyperparameter. Each row of this matrix corresponds to a local region of the input feature map with size kernel_size x kernel_size.</li>



<li>It applies three linear projection layers to this matrix to get the queries, keys and values matrices of size (batch_size * height_out * width_out) x (out_channels), where out_channels is another hyperparameter. Each row of these matrices corresponds to a query, key or value vector for a local region of the input feature map.</li>



<li>It applies the softmax function to the dot product of the queries and keys matrices divided by the square root of out_channels to get the attention weights matrix of size (batch_size * height_out * width_out) x (batch_size * height_out * width_out). Each element of this matrix corresponds to the attention weight between two local regions of the input feature map.</li>



<li>It applies the dot product of the attention weights matrix and the values matrix to get the refined output feature map matrix of size (batch_size * height_out * width_out) x (out_channels). Each row of this matrix corresponds to a refined output feature vector for a local region of the output feature map.</li>
</ul>
</li>



<li>Third, it adds the refined output feature map matrix and the original output feature map matrix to get the final output feature map matrix. This operation combines the information from both sources and enhances the representation.</li>



<li>Fourth, it reshapes the final output feature map matrix to a tensor of size (batch_size) x (out_channels) x (height_out) x (width_out) and returns it as the final output.</li>
</ul>



<p>The attention upsampling method works by using attention to capture long-range dependencies between different regions of the input feature map and refine the output feature map accordingly. It also preserves some information from the original upsampling operation and avoids losing too much resolution. This way, it can produce high-quality outputs from low-resolution inputs.</p>



<p>13de20</p>



<p>Mensaje enviado. Do this kind of methods, all the ones we have been discussing in this chat, innovate about position encoding?</p>



<p>I think some of these methods do innovate about position encoding, while others do not. Position encoding is a way to inject some information about the relative or absolute position of the elements in a sequence into the model. For example, in natural language processing, position encoding is often used to indicate the order of the words in a sentence. In computer vision, position encoding is often used to indicate the spatial location of the pixels or regions in an image.</p>



<p>Some of the methods we have been discussing in this chat do not use any explicit position encoding, but rely on the implicit position information encoded by the spatial location of the feature map pixels or regions. For example, convolution and pooling operations are shift-invariant, meaning that they do not change their output if the input is shifted by some amount. This implies that they do not use any explicit position encoding, but only use the implicit position information encoded by the spatial location of the feature map pixels or regions. Similarly, self-attention and squeeze-and-excitation operations are permutation-invariant, meaning that they do not change their output if the input is permuted by some order. This implies that they do not use any explicit position encoding, but only use the implicit position information encoded by the order of the feature map pixels or regions.</p>



<p>Other methods we have been discussing in this chat do use some explicit position encoding, either by adding some extra information to the input or by modifying the existing operations to make them sensitive to position. For example, oversized convolution and kernel attention convolution use a larger kernel size than the input size to break the shift-invariance property of convolution and introduce some implicit positional encoding. This makes the effective kernels at different spatial locations different when the kernel size is larger than the input size. Another example is attention upsampling, which uses a query-key-value mechanism to compute attention weights from the input feature map. This mechanism can be seen as a form of explicit position encoding, since it uses different queries and keys for different regions of the input feature map.</p>



<p>Therefore, I think some of these methods do innovate about position encoding, while others do not. Position encoding is an important aspect of deep neural network models, especially for tasks that require spatial reasoning or sequential modeling. Different methods may have different ways to encode position information, either implicitly or explicitly, and may have different effects on the performance and efficiency of the models.</p>



<p>14de20</p>



<p>Mensaje enviado. tell about the ones that use new ideas for position encoding, compared with the original trigonometric or learned positions</p>



<p>Buscando: ‚Äò<code>new ideas for position encoding</code>‚Äô</p>



<p>Generando respuestas para usted‚Ä¶</p>



<p>Sure, I can tell you about some of the methods that use new ideas for position encoding, compared with the original trigonometric or learned positions. Here are some examples:</p>



<ul>
<li>
<a><strong>Conditional Positional Encoding (CPE)</strong>&nbsp;</a><a href="https://arxiv.org/pdf/2102.10882.pdf" target="_blank" rel="noreferrer noopener"><sup>1</sup></a>: This method uses a position encoding generator (PEG) to dynamically generate position encodings that are conditioned on the local neighborhood of the input tokens. This way, the position encodings are not predefined and independent of input tokens, but are adaptive and flexible. CPE can easily generalize to the input sequences that are longer than what the model has seen during training, and can also keep the desired translation equivalence in vision tasks.</li>



<li>
<a><strong>Rotary Embedding</strong>&nbsp;</a><a href="https://paperswithcode.com/methods/category/position-embeddings" target="_blank" rel="noreferrer noopener"><sup>2</sup></a>: This method uses a rotation matrix to encode the absolute position information into the phase of the input tokens. This way, the position information is preserved after applying self-attention, and can be recovered by applying an inverse rotation matrix. Rotary embedding can capture both relative and absolute position information, and can also handle variable-length inputs without padding or truncation.</li>



<li>
<a><strong>ALiBi</strong>&nbsp;</a><a href="https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/" target="_blank" rel="noreferrer noopener"><sup>3</sup></a>: This method uses linear biases to encode the relative position information into the attention scores. This way, the position information is added as a bias term to the dot product of queries and keys, and can be learned by gradient descent. ALiBi can capture long-range dependencies without using sinusoidal functions or learnable embeddings, and can also extrapolate to longer inputs than seen during training.</li>
</ul>



<p>These are some of the methods that use new ideas for position encoding, compared with the original trigonometric or learned positions. They have different advantages and disadvantages, and may be suitable for different tasks or scenarios.</p>



<p>M√°s informaci√≥n:</p>



<p><a target="_blank" href="https://arxiv.org/pdf/2102.10882.pdf" rel="noreferrer noopener">1. arxiv.org</a><a target="_blank" href="https://paperswithcode.com/methods/category/position-embeddings" rel="noreferrer noopener">2. paperswithcode.com</a><a target="_blank" href="https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/" rel="noreferrer noopener">3. machinelearningmastery.com</a><a target="_blank" href="https://kazemnejad.com/blog/transformer_architecture_positional_encoding/" rel="noreferrer noopener">4. kazemnejad.com</a>+2&nbsp;m√°s</p>



<p>15de20</p>



<p>Mensaje enviado. what if the features are events and then the position encoding is the time of the event? Is there some different approach to ¬´position-encode¬ª using time? It would condider that elements very near in time are not really sequence, but they could had happened in different order.</p>



<p>Buscando: ‚Äò<code>position encoding time events</code>‚Äô</p>



<p>Generando respuestas para usted‚Ä¶</p>



<p>That‚Äôs a good question. I think there are some different approaches to position-encode using time for events. Here are some examples:</p>



<ul>
<li>
<a><strong>Time2Vec</strong>&nbsp;</a><a href="https://stackoverflow.com/questions/68477306/positional-encoding-for-time-series-based-data-for-transformer-dnn-models" target="_blank" rel="noreferrer noopener"><sup>1</sup></a>: This method uses a vector representation of time that consists of a linear and a periodic component. The linear component captures the absolute time information, while the periodic component captures the cyclic patterns of time. The periodic component is computed by applying a sinusoidal function with learnable parameters to the time value. Time2Vec can encode both continuous and discrete time values, and can be used as an input feature or an attention mechanism for event-based models.</li>



<li>
<a href="https://ai.stackexchange.com/questions/35547/positional-encoding-of-time-series-features" target="_blank" rel="noreferrer noopener"><strong>Time-aware Positional Encoding (TAPE)</strong>&nbsp;</a><a href="https://ai.stackexchange.com/questions/35547/positional-encoding-of-time-series-features" target="_blank" rel="noreferrer noopener"><sup>2</sup></a>: This method uses a positional encoding that incorporates both the relative position and the absolute time information of the events in a sequence. The relative position is encoded by using the standard sinusoidal function with fixed frequencies, while the absolute time is encoded by using another sinusoidal function with learnable frequencies. TAPE can capture both the order and the timing of the events, and can be used as an input feature or an attention mechanism for event-based models.</li>



<li>
<a href="https://arxiv.org/pdf/2210.09455" target="_blank" rel="noreferrer noopener"><strong>Time-aware Self-Attention (TSA)</strong>&nbsp;</a><a href="https://arxiv.org/pdf/2210.09455" target="_blank" rel="noreferrer noopener"><sup>3</sup></a>: This method uses a self-attention mechanism that incorporates both the relative position and the absolute time information of the events in a sequence. The relative position is encoded by using a learnable embedding matrix, while the absolute time is encoded by using a learnable scalar parameter. TSA can capture both the order and the timing of the events, and can be used as an attention mechanism for event-based models.</li>
</ul>



<p>These are some of the methods that use different approaches to position-encode using time for events. They have different advantages and disadvantages, and may be suitable for different tasks or scenarios.</p>



<p>M√°s informaci√≥n:</p>



<p><a target="_blank" href="https://stackoverflow.com/questions/68477306/positional-encoding-for-time-series-based-data-for-transformer-dnn-models" rel="noreferrer noopener">1. stackoverflow.com</a><a target="_blank" href="https://ai.stackexchange.com/questions/35547/positional-encoding-of-time-series-features" rel="noreferrer noopener">2. ai.stackexchange.com</a><a target="_blank" href="https://arxiv.org/pdf/2210.09455" rel="noreferrer noopener">3. arxiv.org</a></p>



<p>16de20</p>



<p>Mensaje enviado. lets go back to the downsample and upsample layers. Is there some way for the network to decide by itself the scale and way of downsampling and upsampling. It seems possible for time encoding, as the next layer could be in fact in different points and use more or less effective tokens.</p>



<p>Buscando: ‚Äò<code>adaptive downsample upsample</code>‚Äô</p>



<p>Generando respuestas para usted‚Ä¶</p>



<p>I think there are some ways for the network to decide by itself the scale and way of downsampling and upsampling. Here are some examples:</p>



<ul>
<li>
<a><strong>Adaptive Pooling</strong>&nbsp;</a><a href="https://pytorch.org/docs/stable/generated/torch.nn.Upsample.html" target="_blank" rel="noreferrer noopener"><sup>1</sup></a>: This method uses a pooling layer that can adapt its output size to a given target size. For example, if the input feature map has size 32 x 32 and the target size is 16 x 16, then the adaptive pooling layer will apply a 2 x 2 pooling operation with stride 2. If the target size is 8 x 8, then the adaptive pooling layer will apply a 4 x 4 pooling operation with stride 4. This way, the adaptive pooling layer can downsample the input feature map to any desired size without losing information or introducing artifacts.</li>



<li>
<a><strong>Adaptive Convolution</strong>&nbsp;</a><a href="https://en.wikipedia.org/wiki/Upsampling" target="_blank" rel="noreferrer noopener"><sup>2</sup></a>: This method uses a convolution layer that can adapt its kernel size and stride to a given target size. For example, if the input feature map has size 32 x 32 and the target size is 16 x 16, then the adaptive convolution layer will use a kernel size of 3 x 3 and a stride of 2. If the target size is 8 x 8, then the adaptive convolution layer will use a kernel size of 5 x 5 and a stride of 4. This way, the adaptive convolution layer can downsample or upsample the input feature map to any desired size while preserving some spatial information and reducing the number of parameters.</li>



<li>
<a><strong>Adaptive Transposed Convolution</strong>&nbsp;</a><a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/UpSampling2D" target="_blank" rel="noreferrer noopener"><sup>3</sup></a>: This method uses a transposed convolution layer that can adapt its kernel size and stride to a given target size. For example, if the input feature map has size 8 x 8 and the target size is 16 x 16, then the adaptive transposed convolution layer will use a kernel size of 3 x 3 and a stride of 2. If the target size is 32 x 32, then the adaptive transposed convolution layer will use a kernel size of 5 x 5 and a stride of 4. This way, the adaptive transposed convolution layer can upsample the input feature map to any desired size while preserving some spatial information and reducing the number of parameters.</li>
</ul>



<p>These are some of the methods that can allow the network to decide by itself the scale and way of downsampling and upsampling. They have different advantages and disadvantages, and may be suitable for different tasks or scenarios.</p>



<p>M√°s informaci√≥n:</p>



<p><a target="_blank" href="https://pytorch.org/docs/stable/generated/torch.nn.Upsample.html" rel="noreferrer noopener">1. pytorch.org</a><a target="_blank" href="https://en.wikipedia.org/wiki/Upsampling" rel="noreferrer noopener">2. en.wikipedia.org</a><a target="_blank" href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/UpSampling2D" rel="noreferrer noopener">3. tensorflow.org</a></p>



<p>17de20</p>



<p>Mensaje enviado. elaborate about Adaptive Attention. Look for work done on this line, but also feel free to contribute creatively.</p>



<p>Buscando: ‚Äò<code>adaptive attention</code>‚Äô</p>



<p>Generando respuestas para usted‚Ä¶</p>



<p>Adaptive attention is a concept that refers to the ability of a model to adjust its attention mechanism according to the input or the task. Adaptive attention can be applied to different domains, such as image captioning, object detection, natural language processing, etc. Here are some examples of works done on this line:</p>



<ul>
<li>
<a><strong>Knowing When to Look: Adaptive Attention via A Visual Sentinel for Image Captioning</strong>&nbsp;</a><a href="https://bing.com/search?q=adaptive+attention" target="_blank" rel="noreferrer noopener"><sup>1</sup></a>: This work proposes a novel adaptive attention model for image captioning that can decide when to attend to the image and when to rely on the language model. The model uses a visual sentinel that acts as a gate to control the information flow from the image and the language model. The visual sentinel is learned jointly with the attention mechanism and the language model. The model can generate more diverse and accurate captions than previous methods.</li>



<li>
<a><strong>Object detection based on an adaptive attention mechanism</strong>&nbsp;</a><a href="https://www.nature.com/articles/s41598-020-67529-x" target="_blank" rel="noreferrer noopener"><sup>2</sup></a>: This work proposes a novel adaptive attention mechanism for object detection that can adaptively emphasize informative features and suppress less useful ones. The model uses three types of attention units: adaptive channel-wise attention, adaptive spatial-wise attention, and adaptive domain attention. The adaptive channel-wise attention can learn the importance of each channel for different tasks. The adaptive spatial-wise attention can learn the spatial distribution of objects and backgrounds. The adaptive domain attention can learn the domain-specific features for different datasets. The model can improve the performance and efficiency of object detection.</li>



<li>
<a><strong>Adaptive Attention Span in Transformers</strong>&nbsp;</a><a href="https://arxiv.org/abs/1612.01887" target="_blank" rel="noreferrer noopener"><sup>3</sup></a>: This work proposes a novel adaptive attention span mechanism for transformers that can dynamically adjust the length of the attention span for each head and each layer. The model uses a learnable scalar parameter to control the decay rate of the attention weights over the distance. The model can learn to attend to longer or shorter contexts depending on the task and the input. The model can reduce the memory and computation cost of transformers while achieving competitive results.</li>
</ul>



<p>These are some of the works done on adaptive attention, but there may be other works as well. Creatively, I think adaptive attention is a promising direction for improving the performance and efficiency of various models. Some possible ideas for future work are:</p>



<ul>
<li>
<strong>Adaptive Attention for Few-shot Learning</strong>: This idea is to use adaptive attention for few-shot learning, which is a task of learning new concepts from few examples. The idea is to use an adaptive attention mechanism that can learn to focus on the most relevant features and examples for each new concept. The adaptive attention mechanism can be based on meta-learning or self-attention, and can be applied to different few-shot learning methods, such as prototypical networks or relation networks.</li>



<li>
<strong>Adaptive Attention for Multimodal Fusion</strong>: This idea is to use adaptive attention for multimodal fusion, which is a task of integrating information from different modalities, such as text, image, audio, etc. The idea is to use an adaptive attention mechanism that can learn to weigh and align different modalities according to their relevance and complementarity. The adaptive attention mechanism can be based on cross-modal or co-attention, and can be applied to different multimodal fusion tasks, such as visual question answering or image captioning.</li>



<li>
<strong>Adaptive Attention for Graph Neural Networks</strong>: This idea is to use adaptive attention for graph neural networks, which are models that can process graph-structured data, such as social networks or knowledge graphs. The idea is to use an adaptive attention mechanism that can learn to assign different importance and directionality to different edges and nodes in a graph. The adaptive attention mechanism can be based on graph convolution or graph attention, and can be applied to different graph neural network tasks, such as node classification or link prediction.</li>
</ul>



<p>M√°s informaci√≥n:</p>



<p><a target="_blank" href="https://bing.com/search?q=adaptive+attention" rel="noreferrer noopener">1. bing.com</a><a target="_blank" href="https://www.nature.com/articles/s41598-020-67529-x" rel="noreferrer noopener">2. nature.com</a><a target="_blank" href="https://arxiv.org/abs/1612.01887" rel="noreferrer noopener">3. arxiv.org</a><a target="_blank" href="https://zhuanlan.zhihu.com/p/389841050" rel="noreferrer noopener">4. zhuanlan.zhihu.com</a><a target="_blank" href="https://blog.csdn.net/qq_31347869/article/details/88554440" rel="noreferrer noopener">5. blog.csdn.net</a>+1&nbsp;m√°s</p>



<p>18de20</p>



<p>Mensaje enviado. adaptive attention should allow for downsampling and upsampling, for instance when tokens are a timed series of events, but also when tokens are values of a time evolving funtion so adaptation can approach and interpolate.</p>



<p>I agree that adaptive attention should allow for downsampling and upsampling, especially when dealing with time series data. Downsampling and upsampling can be useful for reducing the complexity and noise of the data, or for increasing the resolution and detail of the data. Adaptive attention can help to decide when and how to perform downsampling and upsampling, based on the input or the task. For example, adaptive attention can learn to downsample the data when the events are sparse or redundant, or to upsample the data when the events are dense or missing. Adaptive attention can also learn to interpolate the data when the tokens are values of a time evolving function, by using different attention weights or mechanisms. This way, adaptive attention can improve the performance and efficiency of the model by adjusting the scale and way of processing the data.</p>



<p>19de20</p>



<p>Mensaje enviado. Can you show me some code snippets of how to implement adaptive attention for downsampling and upsampling?</p>



<p>Buscando: ‚Äò<code>adaptive attention for downsampling and upsampling code</code>‚Äô</p>



<p>Generando respuestas para usted‚Ä¶</p>



<p>Sure, I can show you some code snippets of how to implement adaptive attention for downsampling and upsampling. Here are some examples:</p>



<ul>
<li>
<strong>Adaptive Attention for Downsampling</strong>: This example uses an adaptive attention mechanism to downsample a time series of events based on their importance and redundancy. The adaptive attention mechanism is based on a self-attention layer that computes the attention weights from the input sequence. The attention weights are then used to select a subset of the input sequence that has the most information and the least noise. The code is written in PyTorch:</li>
</ul>


<div class="wp-block-syntaxhighlighter-code "><pre class="brush: python; title: ; notranslate" title="">
import torch
import torch.nn as nn

class AdaptiveAttentionDownsampling(nn.Module):
    def __init__(self, input_size, output_size, dropout=0.1):
        super(AdaptiveAttentionDownsampling, self).__init__()
        self.input_size = input_size # the size of each input event
        self.output_size = output_size # the desired size of the output sequence
        self.dropout = dropout # the dropout rate for regularization
        # initialize the query, key and value projection layers for self-attention
        self.query_layer = nn.Linear(input_size, input_size)
        self.key_layer = nn.Linear(input_size, input_size)
        self.value_layer = nn.Linear(input_size, input_size)
        # initialize the dropout layer for regularization
        self.dropout_layer = nn.Dropout(dropout)

    def forward(self, x):
        # x is a tensor of shape (batch_size, seq_len, input_size) containing a batch of input sequences
        # get the batch size and the sequence length
        batch_size, seq_len, _ = x.size()
        # apply the query, key and value projection layers to get the queries, keys and values tensors of shape (batch_size, seq_len, input_size)
        queries = self.query_layer(x)
        keys = self.key_layer(x)
        values = self.value_layer(x)
        # transpose the queries and keys tensors to get tensors of shape (batch_size, input_size, seq_len)
        queries = queries.transpose(1, 2)
        keys = keys.transpose(1, 2)
        # apply the softmax function to the dot product of queries and keys tensors divided by the square root of input_size to get the attention weights tensor of shape (batch_size, seq_len, seq_len)
        attention_weights = torch.softmax(torch.matmul(queries, keys) / torch.sqrt(torch.tensor(self.input_size)), dim=-1)
        # apply the dropout layer to the attention weights tensor for regularization
        attention_weights = self.dropout_layer(attention_weights)
        # apply the dot product of the attention weights and values tensors to get the output tensor of shape (batch_size, seq_len, input_size)
        output = torch.matmul(attention_weights, values)
        # sort the output tensor by the sum of attention weights along the sequence dimension in descending order to get the sorted output tensor and the indices tensor of shape (batch_size, seq_len, input_size) and (batch_size, seq_len) respectively
        sorted_output, indices = torch.sort(torch.sum(attention_weights, dim=1), dim=1, descending=True)
        # select the top output_size elements from the sorted output tensor along the sequence dimension to get the final output tensor of shape (batch_size, output_size, input_size)
        final_output = sorted_output&#x5B;:, :output_size]
        # return the final output tensor
        return final_output

</pre></div>


<ul>
<li>
<strong>Adaptive Attention for Upsampling</strong>: This example uses an adaptive attention mechanism to upsample a time series of events based on their relevance and complementarity. The adaptive attention mechanism is based on a cross-attention layer that computes the attention weights from a high-resolution sequence and a low-resolution sequence. The attention weights are then used to interpolate the low-resolution sequence with information from the high-resolution sequence. The code is written in PyTorch:</li>
</ul>


<div class="wp-block-syntaxhighlighter-code "><pre class="brush: python; title: ; notranslate" title="">
import torch
import torch.nn as nn

class AdaptiveAttentionUpsampling(nn.Module):
    def __init__(self, input_size, output_size, dropout=0.1):
        super(AdaptiveAttentionUpsampling, self).__init__()
        self.input_size = input_size # the size of each input event
        self.output_size = output_size # the desired size of the output sequence
        self.dropout = dropout # the dropout rate for regularization
        # initialize the query and key projection layers for cross-attention
        self.query_layer = nn.Linear(input_size, input_size)
        self.key_layer = nn.Linear(input_size, input_size)
        # initialize the dropout layer for regularization
        self
</pre></div>


<p>M√°s informaci√≥n:</p>



<p><a target="_blank" href="https://arxiv.org/abs/2012.09904" rel="noreferrer noopener">1. arxiv.org</a><a target="_blank" href="https://arxiv.org/abs/2302.06096" rel="noreferrer noopener">2. arxiv.org</a><a target="_blank" href="https://pytorch.org/docs/stable/generated/torch.nn.Upsample.html" rel="noreferrer noopener">3. pytorch.org</a></p>



<p>20de20</p>



<p>Esta conversaci√≥n ha alcanzado su l√≠mite. Usa el bot√≥n ¬´escoba¬ª para borrar esto y chatear m√°s.</p>
</div>


<div class="wp-block-group has-global-padding is-layout-constrained wp-block-group-is-layout-constrained" style="margin-top:var(--wp--preset--spacing--40);padding-bottom:var(--wp--preset--spacing--50)">


<div class="wp-block-group has-global-padding is-layout-constrained wp-block-group-is-layout-constrained">
<div style="height:var(--wp--preset--spacing--40)" aria-hidden="true" class="wp-block-spacer"></div>



<hr class="wp-block-separator has-text-color has-contrast-3-color has-alpha-channel-opacity has-contrast-3-background-color has-background is-style-wide" style="margin-bottom:var(--wp--preset--spacing--40)">



<div class="wp-block-comments wp-block-comments-query-loop">
<h2 class="wp-block-heading">Comments</h2>








	<div id="respond" style="padding-top:var(--wp--preset--spacing--20);padding-bottom:var(--wp--preset--spacing--20);" class="comment-respond wp-block-post-comments-form">
		<h3 id="reply-title" class="comment-reply-title">Deja una respuesta</h3>
<form action="https://a.rivero.nom.es/wp-comments-post.php" method="post" id="commentform" class="comment-form" novalidate>
<p class="comment-notes"><span id="email-notes">Tu direcci√≥n de correo electr√≥nico no ser√° publicada.</span> <span class="required-field-message">Los campos obligatorios est√°n marcados con <span class="required">*</span></span></p>
<p class="comment-form-comment"><label for="comment">Comentario <span class="required">*</span></label> <textarea id="comment" name="comment" cols="45" rows="8" maxlength="65525" required></textarea></p>
<p class="comment-form-author"><label for="author">Nombre</label> <input id="author" name="author" type="text" value="" size="30" maxlength="245" autocomplete="name"></p>
<p class="comment-form-email"><label for="email">Correo electr√≥nico</label> <input id="email" name="email" type="email" value="" size="30" maxlength="100" aria-describedby="email-notes" autocomplete="email"></p>
<p class="comment-form-url"><label for="url">Web</label> <input id="url" name="url" type="url" value="" size="30" maxlength="200" autocomplete="url"></p>
<p class="form-submit wp-block-button"><input name="submit" type="submit" id="submit" class="wp-block-button__link wp-element-button" value="Publicar el comentario"> <input type="hidden" name="comment_post_ID" value="2111" id="comment_post_ID">
<input type="hidden" name="comment_parent" id="comment_parent" value="0">
</p>
<p style="display: none;"><input type="hidden" id="akismet_comment_nonce" name="akismet_comment_nonce" value="ad6cd9823e"></p>
<p style="display: none !important;" class="akismet-fields-container" data-prefix="ak_"><label>&#916;<textarea name="ak_hp_textarea" cols="45" rows="8" maxlength="100"></textarea></label><input type="hidden" id="ak_js_1" name="ak_js" value="85"><script>document.getElementById( "ak_js_1" ).setAttribute( "value", ( new Date() ).getTime() );</script></p>
</form>	</div>
<!-- #respond -->
	<p class="akismet_comment_form_privacy_notice">Este sitio usa Akismet para reducir el spam. <a href="https://akismet.com/privacy/" target="_blank" rel="nofollow noopener">Aprende c√≥mo se procesan los datos de tus comentarios</a>.</p>
</div>



<nav aria-label="Posts" class="wp-block-group is-content-justification-space-between is-nowrap is-layout-flex wp-container-core-group-is-layout-10 wp-block-group-is-layout-flex" style="padding-top:var(--wp--preset--spacing--40);padding-bottom:var(--wp--preset--spacing--40)"><div class="post-navigation-link-previous wp-block-post-navigation-link">
<span class="wp-block-post-navigation-link__arrow-previous is-arrow-arrow" aria-hidden="true">‚Üê</span><a href="/blog/bing-thinks-sort-of-about-numerical-integration-and-derivation-with-transformers/" rel="prev"><span class="post-navigation-link__label">Previous: </span> <span class="post-navigation-link__title">Bing thinks -sort of- about numerical Integration and Derivation with transformers</span></a>
</div>

<div class="post-navigation-link-next wp-block-post-navigation-link">
<a href="/blog/la-simplicidad-de-la-formula-de-koide/" rel="next"><span class="post-navigation-link__label">Next: </span> <span class="post-navigation-link__title">La simplicidad de la formula de Koide</span></a><span class="wp-block-post-navigation-link__arrow-next is-arrow-arrow" aria-hidden="true">‚Üí</span>
</div></nav>
</div>
</div>
</main>
</div>
<script src="/blog/wp-content/plugins/syntaxhighlighter/syntaxhighlighter3/scripts/shCore.js?ver=3.0.9b" id="syntaxhighlighter-core-js"></script>
<script src="/blog/wp-content/plugins/syntaxhighlighter/syntaxhighlighter3/scripts/shBrushPython.js?ver=3.0.9b" id="syntaxhighlighter-brush-python-js"></script>
<script type="text/javascript">(function(){
		var corecss = document.createElement('link');
		var themecss = document.createElement('link');
		var corecssurl = "/blog/wp-content/plugins/syntaxhighlighter/syntaxhighlighter3/styles/shCore.css?ver=3.0.9b";
		if ( corecss.setAttribute ) {
				corecss.setAttribute( "rel", "stylesheet" );
				corecss.setAttribute( "type", "text/css" );
				corecss.setAttribute( "href", corecssurl );
		} else {
				corecss.rel = "stylesheet";
				corecss.href = corecssurl;
		}
		document.head.appendChild( corecss );
		var themecssurl = "/blog/wp-content/plugins/syntaxhighlighter/syntaxhighlighter3/styles/shThemeDefault.css?ver=3.0.9b";
		if ( themecss.setAttribute ) {
				themecss.setAttribute( "rel", "stylesheet" );
				themecss.setAttribute( "type", "text/css" );
				themecss.setAttribute( "href", themecssurl );
		} else {
				themecss.rel = "stylesheet";
				themecss.href = themecssurl;
		}
		document.head.appendChild( themecss );
	})();
	SyntaxHighlighter.config.strings.expandSource = '+ expandir c√≥digo';
	SyntaxHighlighter.config.strings.help = '?';
	SyntaxHighlighter.config.strings.alert = 'SyntaxHighlighter\n\n';
	SyntaxHighlighter.config.strings.noBrush = 'No se encuentra el pincel para:';
	SyntaxHighlighter.config.strings.brushNotHtmlScript = 'El pincel no est√° configurado para la opci√≥n html-script:';
	SyntaxHighlighter.defaults['gutter'] = false;
	SyntaxHighlighter.defaults['pad-line-numbers'] = false;
	SyntaxHighlighter.defaults['toolbar'] = false;
	SyntaxHighlighter.all();

	// Infinite scroll support
	if ( typeof( jQuery ) !== 'undefined' ) {
		jQuery( function( $ ) {
			$( document.body ).on( 'post-load', function() {
				SyntaxHighlighter.highlight();
			} );
		} );
	}</script>
<script defer src="/blog/wp-content/plugins/akismet/_inc/akismet-frontend.js?ver=1711023483" id="akismet-frontend-js"></script>
<script src="/blog/wp-includes/js/comment-reply.min.js?ver=6.5.3" id="comment-reply-js" async data-wp-strategy="async"></script>
<script id="wp-block-template-skip-link-js-after">( function() {
		var skipLinkTarget = document.querySelector( 'main' ),
			sibling,
			skipLinkTargetID,
			skipLink;

		// Early exit if a skip-link target can't be located.
		if ( ! skipLinkTarget ) {
			return;
		}

		/*
		 * Get the site wrapper.
		 * The skip-link will be injected in the beginning of it.
		 */
		sibling = document.querySelector( '.wp-site-blocks' );

		// Early exit if the root element was not found.
		if ( ! sibling ) {
			return;
		}

		// Get the skip-link target's ID, and generate one if it doesn't exist.
		skipLinkTargetID = skipLinkTarget.id;
		if ( ! skipLinkTargetID ) {
			skipLinkTargetID = 'wp--skip-link--target';
			skipLinkTarget.id = skipLinkTargetID;
		}

		// Create the skip link.
		skipLink = document.createElement( 'a' );
		skipLink.classList.add( 'skip-link', 'screen-reader-text' );
		skipLink.href = '#' + skipLinkTargetID;
		skipLink.innerHTML = 'Saltar al contenido';

		// Inject the skip link.
		sibling.parentElement.insertBefore( skipLink, sibling );
	}() );</script>
<script src="/blog/wp-content/plugins/page-links-to/dist/new-tab.js?ver=3.3.7" id="page-links-to-js"></script>
</body>
</html>